#+TITLE: 深度学习论文的笔记摘抄

* FCN 中的FC是什么含义？
Fully Convolution意味着什么？
FCN 一开始就和语义分割扯上关系，怎么回事？
* Siamese and Triplet Net 在 Image Ebedding 中的作用
https://github.com/adambielski/siamese-triplet

* FPN 的目的是什么？
源自文章 "Feature pyramid networks for object detection"
各种尺度的Feature Map的来源,及相互关系,利用方式! 那么下面的术语:
- Featurized Image Pyramid: Image Pyramid 产生 Feature Map堆栈
- Single/Top Feature map: 只利用DNN的顶层Fetaure Map
- Pyramidical Feature Hierarchy: 独立利用DNN的各层Feature Map
- Feature Pyramid Networks: 是种DNN范式。按我的理解,具有"生成"的味道?
* Convolutional Activition Map 是个什么概念？
CAM: Learning Deep Features for Discriminative Localization
Grad-cam: Visual explanations from deep networks via gradient-based localization
各自说了什么事？

* Wieland Brendel and Matthias Bethge 想说什么?
** 文章ImageNet-Trained CNNs are biased towards texture; Increasing shape bias improves accuracy and robustness.
用标题将他的内容全部说出来了.
** 文章BagNet
进一步将上面的事说明白了!
但我将他们的代码从头实现一遍,总觉得:造做！
* Fine Grained Image Analysis
** 纲要
*** Fine-grained image recognition
**** Fine-grained recognition by localization-classification subnetworks
***** Employing detection or segmentation techniques
***** Utilizing deep filters / activations
***** Leveraging attention mechanisms
***** Other methods
**** Fine-grained recognition by end-to-end feature encoding
***** High-order feature interactions
***** Specific loss functions
***** Other methods
**** Fine-grained recognition with external information
***** Fine-grained recognition with web data
***** Fine-grained recognition with multi-modality data
***** Fine-grained recognition with humans in the loop
*** Fine-grained image retrieval
**** Content-based fine-grained image retrieval
**** Sketch-based fine-grained image retrieval
*** Fine-grained image generation
**** Generating from fine-grained image distributions
**** Generating from text descriptions
*** Future directions of FGIA
**** Fine-grained few shot learning
**** Fine-grained hashing
**** Fine-grained domain adaptation
**** FGIA within more realistic settings
** 论文阅读一：视觉对象的图表示及可解释性
2020 - Interpretable and Accurate Fine-grained Recognition via Region Grouping
2018 - Beyond Grids: Learning Graph Representations for Visual Recognition
2018年的文章应是这条路线的基础。基于Perceptual Grouping的观点，运用Graph Representation方法，去研究视觉对象的认知问题。
* Anchor Free Object Detection
这中间的理论问题是什么？最终追溯到“对象的表示”问题。
边界框是目前最通用的，但它提供的信息实在是太粗旷了。
什么叫合理或合适的对象表示？
** Paper List
- 2020
  - Any More?
  - AutoAssign: Differentiable Label Assignment for Dense Object Detection.(Rejected?)
  - RepPoints V2: Verification Meets Regression for Object Detection.(*Pay Attention!*)
  - Corner Proposal Network for Anchor-free, Two-stage Object Detection.
  - HoughNet: Integrating near and long-range evidence for bottom-up object detection.
  - Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection.
  - Soft Anchor-Point Object Detection.
  - CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection.
  - SaccadeNet: A Fast and Accurate Object Detector.
  - Localization Uncertainty Estimation for Anchor-Free Object Detection.
  - Dense RepPoints: Representing Visual Objects with Dense Point Sets.(*Pay Attention!*)
  - BorderDet: Border Feature for Dense Object Detection.
  - Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.
- 2019
  - RepPoints: Point Set Representation for Object Detection.(*Pay Attention*)
  - Segmentation is All You Need.
  - FCOS: Fully Convolutional One-Stage Object Detection. (*Pay Attention*)
    [[file:./images/fcos_Architecture.png]]
  - CenterNet: Detecting Objects as Paired Points
  - CenterNet: Keypoint Triplets for Object Detection.
  - CenterNet: Objects as Points. (*Pay Attention*)
  - FoveaBox: Beyond Anchor-based Object Detector.
  - Feature Selective Anchor-Free Module for Single-Shot Object Detection.
  - ExtremeNet: Bottom-up Object Detection by Grouping Extreme and Center Points.
- 2018
  - CornerNet: Detecting Objects as Paired Keypoints.
  - An Anchor-Free Region Proposal Network for Faster R-CNN based Text Detection Approaches.
- 2016
  - You Only Look Once: Unified, Real-Time Object Detection.
  - UnitBox: An Advanced Object Detection Network.
- 2015
  - DenseBox: Unifying Landmark Localization with End to End Object Detection.
** Arguments

* CAM和Grad-CAM的涵义:
* Graph Neural Network:
** The Concept of GNN:
** Graph Embeding
** Graph Convolution:
  - Spatial Convolution
  - Spectral Convolution
* DETR 给Object Detection带来的新范式
Detection Transformer(DeTr): End-To-End by Transformers.
Anchor Based --> Anchor Free --> End-to-End!!
** How does DeTr do it?
*** Transformer
*** Learnable Object Queries
*** Set-Prediction

** Some Rethinking
*** Effective Transformer.
Deformable-DeTr(重点看看,打开DeTr的正确方式?),UP-DeTr等等
**** What makes for an effective attention mechanisms?
**** How to aggregate multi-scale features into attention mechanisms?
**** How to train the transformer effectively?
这方面的研究,和在ViT上的很多研究是一致的.

*** What makes for a sparse object detection methods.
Sparse R-CNN等
*** New label-assignment methods.
OTA等,
**** Considering label assignment from a global view.
*** What makes for a end-to-end object detection?
OneNet等
主要还是和label-assignment有关.具体的说,还要考虑什么因素?
*** How to extends to segmentation task?
**** SOLQ(Segmenting Object As Queries)等,还有对Box的考虑
**** MaX-DeepLab则开启了基于Mask Transformer的端到端(全景)分割框架.(MaX指Mask XFormers).
似乎有这样一条线索: *Box Based + Pixel Classification* --> *Box Free + Pixel Classification* --> *Mask Classification*
- MaX-DeepLab --> kMaX-DeepLab
- MaskFormer(For Semantic segmentation) --> Mask2Former (For panoptic segmentation) --> OneFormer (Universe image segmentation)
  + MaX-DeepLab 和 MaxFormer,
  + 很有意思,在争论谁先落实 pixel classification 到 mask classification的转变.
  + Mask Classification 理论上,可以将各种分割任务进行统一.
- SegFormer (For semantic segmentation)--> Panoptic SegFormer (For panoptic segmentation)
- DiNo (For object detetction) --> Mask DiNo (Can object detection and segmentation be unified)
  + 这是在找话题吗?还是真见地?

*** How to apply for transfer learning or domain adaption task?
** DINO家族(此DINO非彼DiNo,不道德的取名方式)
DINO means "Detr with Improved deNoising anchOr boxes"
DAB-DeTr --> DN-DeTr --> DiNo --> Mask DiNo
* 关于Set-Prediction Problem 还可看看下述文章
Deep Sets
Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks
Generative Adversarial Set Transformers
Conditional Set Generation with Transformers
* DiNo 给ViT的带来的新范式
文章2020 - DINO: Emerging Properties in Self-Supervised Vision Transformers
DiNo means "self-Distillation with No labels"
关键问题:学习方法对ViT架构为什么有如此大的影响?导致自监督学习在目前是个研究热点!
** 回顾一下Self-Supervised的历史
(有必要的话,可以考虑一下KD(Knowledge Distillation), 和Self-Supervised之间的碰撞)
*** Pretext Task : "Relative Location","Colorization","Context Encoder","Rotation Prediction"等主题文章.
"Pretext Task":所谓的"前置任务".
*** Contrastive Learning :
**** "MoCo(v1,v2,v3)":MoCo 这系列论文，将之前的对比学习,总结成字典查找的框架,再基于此提出 MoCo(Moment Contrast).

**** "SimCLR":
**** "BYOL(Boot your own latent)":
**** "SwAV(Swapping Assignments between multiple Views )":
**** "SimSiam": SimSiam 这篇论文则是对上述多篇论文进行了总结,并且化繁为简.
**** "BarlowTwins":
**** "DiNo":

*** Masked Image(Data) Modeling:
ViT, 为NLP中的掩码学习机制，应用到视觉学习中， 打开了一扇大门！
**** BEiT(Bidirectional Encoder representation from Image Transformers):
**** MAE(Masked Autoencoders):
**** SimMIM:
**** MaskFeat:
* CSSP: Column Subset Selection Problem
文章Select to Better Learn: Fast and Accurate Deep Learning using Data Selection from Nonlinear Manifolds
* Variants of Transformer
我认为这可能是个重要的话题.Transformer本身,毕竟不是高效的.
[[file:./images/VariantsOfTransformer.jpg]]
Sparse transformer, LongFormer, Switch Transformers :
- *Explicit Sparse Transformer: : Concentrated Attention Through Explicit Selection*
- *Longformer: The Long-Document Transformer*
- *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*
Transformer-XL, Star-Transformer:
- *Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context*
Routing Transformer, Linformer, Big Bird:(其实还是稀疏路线)
- *Efficient Content-Based Sparse Attention with RoutingTransformers*
- *Linformer: Self-Attention with Linear Complexity*
- *Big Bird: Transformers for Longer Sequences*

* 理解大语言模型的一些文章
** Sebastian Ruder这几年老作评论:
*** 2018年, Sebastian Ruder做了一个回顾,比较权威.
- 2001 - 神经语言模型
- 2008 - 多任务学习
- 2013 - 词嵌入
- 2013 - NLP 神经网络
- 2014 - sequence-to-sequence 模型
- 2015 - 注意力机制
- 2015 - 基于记忆的网络
- 2018 - 预训练语言模型
*** 2021年, Sebastian Ruder提到的NLP热点
- Universal Models
- Massive Multi-task Learning
- Beyond the Transformer: (Perceiver..)
- Prompting
- Efficient Methods
- Benchmarking
- Conditional Image Generation
- ML for Science
- Program Synthesis
- Bias
- Retrieval Augmentation
- Token-free Models
- Temporal Adaptation
- The Importance of Data
- Meta-learning
** NLP的10片主干文章
*** 理解大语言模型的结构和任务
- Neural Machine Translation by Jointly Learning to Align and Translate (2014)
  此文,引入了Attention Mechanisms,成为引入Transformer的动机!
- Attention Is All You Need (2017)
  See [[./images/NLP/Attention.jpg]]
  在原始的 Transformer 模型之后，大语言模型研究开始向两个方向分化:
  - 基于编码器结构的Transformer模型用于预测建模任务,例如文本分类;
  - 而基于解码器结构的Transformer模型用于生成建模任务,例如翻译、摘要和其他形式的文本内容生成。
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
  BERT是基于解码器的模型结构!
- Improving Language Understanding by Generative Pre-Training (2018) -- GPT
  GPT是基于解码器的模型结构!
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)
*** 规模法则和模型效率提升
- Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)
- Cramming: Training a Language Model on a Single GPU in One Day (2022)
- Training Compute-Optimal Large Language Models (2022)
*** 对齐——引导大语言模型完成训练目标
- Training Language Models to Follow Instructions with Human Feedback (2022) -- InstructGPT!
- Constitutional AI: Harmlessness from AI Feedback (2022)
*** 关于人类反馈的强化学习(RLHF)
- Asynchronous Methods for Deep Reinforcement Learning (2016)
- Proximal Policy Optimization Algorithms (2017)
- Learning to Summarize from Human Feedback (2022)
** NLP中在PTMs思想下的一般性的神经网络架构
Pre-trained Models for Natural Language Processing: A Survey(2020)
See [[./images/NLP/Embedding.jpg]]
应当说,这篇文章把PTM总结得比较好!
** Prompting的花样年华
Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(2021)
四个NLP的范式,See: [[./images/NLP/prompting.jpeg]]
这篇综述,让Prompting红得发紫.其实这篇文章,应当是个典范.一片综述,让一个模糊概念有了清楚的正式定义,并指明了这个领域的研究范畴.
形式化地讲,在Prompt范式下,需要通过以下三个步骤建立从输入到输出的PipeLine.
- Prompt addition:
- Answer search:
- Answer mapping:
由是而有下面五方面的工作.
- Pre-trained Model Choice
- Prompt Engineering
- Answer Engineering
- Expanding the Paradigm
- Prompt-Based Training Strategies
