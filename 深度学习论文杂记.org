#+TITLE: 深度学习论文的笔记摘抄

* FCN 中的FC是什么含义？
Fully Convolution意味着什么？
FCN 一开始就和语义分割扯上关系，怎么回事？
* Siamese and Triplet Net 在 Image Ebedding 中的作用
https://github.com/adambielski/siamese-triplet

* FPN 的目的是什么？
源自文章 "Feature pyramid networks for object detection"
各种尺度的Feature Map的来源,及相互关系,利用方式! 那么下面的术语:
- Featurized Image Pyramid: Image Pyramid 产生 Feature Map堆栈
- Single/Top Feature map: 只利用DNN的顶层Fetaure Map
- Pyramidical Feature Hierarchy: 独立利用DNN的各层Feature Map
- Feature Pyramid Networks: 是种DNN范式。按我的理解,具有"生成"的味道?
* Convolutional Activition Map 是个什么概念？
CAM: Learning Deep Features for Discriminative Localization
Grad-cam: Visual explanations from deep networks via gradient-based localization
各自说了什么事？

* Wieland Brendel and Matthias Bethge 想说什么?
** 文章ImageNet-Trained CNNs are biased towards texture; Increasing shape bias improves accuracy and robustness.
用标题将他的内容全部说出来了.
** 文章BagNet
进一步将上面的事说明白了!
但我将他们的代码从头实现一遍,总觉得:造做！
* Fine Grained Image Analysis
** 纲要
*** Fine-grained image recognition
**** Fine-grained recognition by localization-classification subnetworks
***** Employing detection or segmentation techniques
***** Utilizing deep filters / activations
***** Leveraging attention mechanisms
***** Other methods
**** Fine-grained recognition by end-to-end feature encoding
***** High-order feature interactions
***** Specific loss functions
***** Other methods
**** Fine-grained recognition with external information
***** Fine-grained recognition with web data
***** Fine-grained recognition with multi-modality data
***** Fine-grained recognition with humans in the loop
*** Fine-grained image retrieval
**** Content-based fine-grained image retrieval
**** Sketch-based fine-grained image retrieval
*** Fine-grained image generation
**** Generating from fine-grained image distributions
**** Generating from text descriptions
*** Future directions of FGIA
**** Fine-grained few shot learning
**** Fine-grained hashing
**** Fine-grained domain adaptation
**** FGIA within more realistic settings
** 论文阅读一：视觉对象的图表示及可解释性
2020 - Interpretable and Accurate Fine-grained Recognition via Region Grouping
2018 - Beyond Grids: Learning Graph Representations for Visual Recognition
2018年的文章应是这条路线的基础。基于Perceptual Grouping的观点，运用Graph Representation方法，去研究视觉对象的认知问题。
* Anchor Free Object Detection
这中间的理论问题是什么？最终追溯到“对象的表示”问题。
边界框是目前最通用的，但它提供的信息实在是太粗旷了。
什么叫合理或合适的对象表示？
** Paper List
- 2020
  - Any More?
  - AutoAssign: Differentiable Label Assignment for Dense Object Detection.(Rejected?)
  - RepPoints V2: Verification Meets Regression for Object Detection.(*Pay Attention!*)
  - Corner Proposal Network for Anchor-free, Two-stage Object Detection.
  - HoughNet: Integrating near and long-range evidence for bottom-up object detection.
  - Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection.
  - Soft Anchor-Point Object Detection.
  - CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection.
  - SaccadeNet: A Fast and Accurate Object Detector.
  - Localization Uncertainty Estimation for Anchor-Free Object Detection.
  - Dense RepPoints: Representing Visual Objects with Dense Point Sets.(*Pay Attention!*)
  - BorderDet: Border Feature for Dense Object Detection.
  - Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.
- 2019
  - RepPoints: Point Set Representation for Object Detection.(*Pay Attention*)
  - Segmentation is All You Need.
  - FCOS: Fully Convolutional One-Stage Object Detection. (*Pay Attention*)
    [[file:./images/fcos_Architecture.png]]
  - CenterNet: Detecting Objects as Paired Points
  - CenterNet: Keypoint Triplets for Object Detection.
  - CenterNet: Objects as Points. (*Pay Attention*)
  - FoveaBox: Beyond Anchor-based Object Detector.
  - Feature Selective Anchor-Free Module for Single-Shot Object Detection.
  - ExtremeNet: Bottom-up Object Detection by Grouping Extreme and Center Points.
- 2018
  - CornerNet: Detecting Objects as Paired Keypoints.
  - An Anchor-Free Region Proposal Network for Faster R-CNN based Text Detection Approaches.
- 2016
  - You Only Look Once: Unified, Real-Time Object Detection.
  - UnitBox: An Advanced Object Detection Network.
- 2015
  - DenseBox: Unifying Landmark Localization with End to End Object Detection.
** Arguments

* CAM和Grad-CAM的涵义:
* Graph Neural Network:
** The Concept of GNN:
** Graph Embeding
** Graph Convolution:
  - Spatial Convolution
  - Spectral Convolution
* DETR 给Object Detection带来的新范式
Detection Transformer(DeTr): End-To-End by Transformers.
Anchor Based --> Anchor Free --> End-to-End!!
** How does DeTr do it?
*** Transformer
*** Learnable Object Queries
*** Set-Prediction

** Some Rethinking
*** Effective Transformer.
Deformable-DeTr(重点看看,打开DeTr的正确方式?),UP-DeTr等等
**** What makes for an effective attention mechanisms?
**** How to aggregate multi-scale features into attention mechanisms?
**** How to train the transformer effectively?
这方面的研究,和在ViT上的很多研究是一致的.
*** Deformable Convolution NetWork 和 Transformer之间的深沉关系.(?都在解决"long-range dependencies and adaptive spatial aggregation"问题?)
InternImage, 按照ViTs的架构,来设计基于DCN的架构, 居然也取得了SOTA.
*** What makes for a sparse object detection methods.
Sparse R-CNN等
*** New label-assignment methods.
OTA等,
**** Considering label assignment from a global view.
*** What makes for a end-to-end object detection?
OneNet等
主要还是和label-assignment有关.具体的说,还要考虑什么因素?
*** How to extends to segmentation task?
**** SOLQ(Segmenting Object As Queries)等,还有对Box的考虑
**** MaX-DeepLab则开启了基于Mask Transformer的端到端(全景)分割框架.(MaX指Mask XFormers).
似乎有这样一条线索: *Box Based + Pixel Classification* --> *Box Free + Pixel Classification* --> *Mask Classification*
- MaX-DeepLab --> kMaX-DeepLab
- MaskFormer(For Semantic segmentation) --> Mask2Former (For panoptic segmentation) --> OneFormer (Universe image segmentation)
  + MaX-DeepLab 和 MaxFormer,
  + 很有意思,在争论谁先落实 pixel classification 到 mask classification的转变.
  + Mask Classification 理论上,可以将各种分割任务进行统一.
- SegFormer (For semantic segmentation)--> Panoptic SegFormer (For panoptic segmentation)
- DiNo (For object detetction) --> Mask DiNo (Can object detection and segmentation be unified)
  + 这是在找话题吗?还是真见地?

*** How to apply for transfer learning or domain adaption task?
** DINO家族(此DINO非彼DiNo,不道德的取名方式)
DINO means "Detr with Improved deNoising anchOr boxes"
DAB-DeTr --> DN-DeTr --> DiNo --> Mask DiNo --> OpenSeeD

* 关于Set-Prediction Problem 还可看看下述文章
Deep Sets
Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks
Generative Adversarial Set Transformers
Conditional Set Generation with Transformers
* DiNo 给ViT的带来的新范式
看来FaceBook或MetaAI在这个方向,有不错的努力.前面还有项工作,DeiT,也作了不少努力!
文章2021 - DINO: Emerging Properties in Self-Supervised Vision Transformers
文章2023 - DINOv2: Learning Robust Visual Features without Supervision (附加 文章2023 - Vision Transformers Need Registers)

DiNo means "self-Distillation with No labels"
Dino是第一个在ViT架构上作自监督学习的!
之前,都是在CNN架构的网络上作自监督学习的.
关键问题:学习方法对ViT架构为什么有如此大的影响?导致自监督学习在目前是个研究热点!
** 回顾一下Self-Supervised的历史
(有必要的话,可以考虑一下KD(Knowledge Distillation), 和Self-Supervised之间的碰撞)
*** Pretext Task : "Relative Location","Colorization","Context Encoder","Rotation Prediction"等主题文章.
"Pretext Task":所谓的"前置任务".
*** Contrastive Learning :
**** "MoCo(v1,v2,v3)":MoCo 这系列论文，将之前的对比学习,总结成字典查找的框架,再基于此提出 MoCo(Moment Contrast).
**** "SimCLR":
**** "BYOL(Boot your own latent)":
**** "SwAV(Swapping Assignments between multiple Views )":
**** "SimSiam": SimSiam 这篇论文则是对上述多篇论文进行了总结,并且化繁为简.
**** "BarlowTwins":
**** "DiNo": "self-supervised learning as a form of Mean Teacher self-distillation with no labels."
这句话怎么理解?

*** Masked Image(Data) Modeling:
ViT, 为NLP中的掩码学习机制，应用到视觉学习中， 打开了一扇大门！
**** BEiT(Bidirectional Encoder representation from Image Transformers):
**** MAE(Masked Autoencoders):
**** SimMIM:
**** MaskFeat:
** Dino
*** 网络架构: Self Distillation Architecture
采用Momentum Tearcher模型.思想来自"Momentum Contrast for Unsupervised Visual Representation Learning".
据说,可有效解决Mode Collapse 问题!
*** 数据增强
DINO 中最核心的数据采样策略便是图像裁剪，这也是自监督学习领域应用非常广泛的主策略之一.
一般来说，我们可以将裁剪后的图像分为两种:
- Local views: 即局部视角，也称为 small crops，指的是抠图面积小于原始图像的 50%.
- Global views: 即全局视角，也称为 large crops，指的是抠图面积大于原始图像的 50%.
在 DINO 中，学生模型接收所有预处理过的crops图，而教师模型仅接收来自global views的crops图。
*** Loss函数
教师和学生得分的Cross Entropy!
*** Centering and Sharpening (可以研究一下,处理模式崩塌的技术)
在自监督学习中，mode collapse 是指网络的学习过程中出现了多样性减少的现象.
具体来说，当网络学习到一组特征表示时，往往会出现多个输入数据映射到相同的特征表示的情况，这就是所谓的模式崩塌.

这种现象通常是由于网络在优化过程中陷入了局部最优解.
只能考虑到一部分数据的特征表示,而忽略了其它数据样本的模式和特征,从而导致了多样性缺失的现象,因此会对模型的鲁棒性产生很大的负面影响.
** DinoV2(偏现有技术的灵活综合运用!)
改善内容:
*** 数据集处理:
去燥(不合格的数据),去重(参考:A self-supervised descriptor for image copy detection) 得到数据集LVD-142M.
*** 方法
**** Discriminative Self-Supervised Method: 可认为是下述(DINO, iBOT, SwAV)三种方法的综合/组合?
- DINO: <==> Image Level Objective
- iBOT: <==> Patch Level Objective
- Sinkhorn-Knopp centering(SwAV):这个方法的核心思想是通过正则化来使学生和教师网络在特征表示上更加接近.
  (出自Dino作者早期的一些工作(其实,我发现DiNoS的工作都和他在Contrastive Learning,Deep Clustering方面的工作有关):
   "2020 - Unsupervised learning of visual features by contrasting cluster assignments",
   "2018 - Deep Clustering for Unsupervised Learning of Visual Features"
  )
- Untying head weights between both image and patch objectives. DiNo易过拟合,iBot易欠拟合.
- KoLeo regularizer:
- Adapting the resolution:
**** Efficient Implementation
这得学习源代码了!

* CSSP: Column Subset Selection Problem
文章Select to Better Learn: Fast and Accurate Deep Learning using Data Selection from Nonlinear Manifolds
* Variants of Transformer
我认为这可能是个重要的话题.Transformer本身,毕竟不是高效的.
[[file:./images/VariantsOfTransformer.jpg]]
Sparse transformer, LongFormer, Switch Transformers :
- *Explicit Sparse Transformer: : Concentrated Attention Through Explicit Selection*
- *Longformer: The Long-Document Transformer*
- *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*
Transformer-XL, Star-Transformer:
- *Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context*
Routing Transformer, Linformer, Big Bird:(其实还是稀疏路线)
- *Efficient Content-Based Sparse Attention with RoutingTransformers*
- *Linformer: Self-Attention with Linear Complexity*
- *Big Bird: Transformers for Longer Sequences*

* 理解大语言模型的一些文章
** Sebastian Ruder这几年老作评论:
*** 2018年, Sebastian Ruder做了一个回顾,比较权威.
- 2001 - 神经语言模型
- 2008 - 多任务学习
- 2013 - 词嵌入
- 2013 - NLP 神经网络
- 2014 - sequence-to-sequence 模型
- 2015 - 注意力机制
- 2015 - 基于记忆的网络
- 2018 - 预训练语言模型
*** 2021年, Sebastian Ruder提到的NLP热点
- Universal Models
- Massive Multi-task Learning
- Beyond the Transformer: (Perceiver..)
- Prompting
- Efficient Methods
- Benchmarking
- Conditional Image Generation
- ML for Science
- Program Synthesis
- Bias
- Retrieval Augmentation
- Token-free Models
- Temporal Adaptation
- The Importance of Data
- Meta-learning
** NLP的10片主干文章
*** 理解大语言模型的结构和任务
- Neural Machine Translation by Jointly Learning to Align and Translate (2014)
  此文,引入了Attention Mechanisms,成为引入Transformer的动机!
- Attention Is All You Need (2017)
  See [[./images/NLP/Attention.jpg]]
  在原始的 Transformer 模型之后，大语言模型研究开始向两个方向分化:
  - 基于编码器结构的Transformer模型用于预测建模任务,例如文本分类;
  - 而基于解码器结构的Transformer模型用于生成建模任务,例如翻译、摘要和其他形式的文本内容生成。
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
  BERT是基于解码器的模型结构!
- Improving Language Understanding by Generative Pre-Training (2018) -- GPT
  GPT是基于解码器的模型结构!
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)
*** 规模法则和模型效率提升
- Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)
- Cramming: Training a Language Model on a Single GPU in One Day (2022)
- Training Compute-Optimal Large Language Models (2022)
*** 对齐——引导大语言模型完成训练目标
- Training Language Models to Follow Instructions with Human Feedback (2022) -- InstructGPT!
- Constitutional AI: Harmlessness from AI Feedback (2022)
*** 关于人类反馈的强化学习(RLHF)
- Asynchronous Methods for Deep Reinforcement Learning (2016)
- Proximal Policy Optimization Algorithms (2017)
- Learning to Summarize from Human Feedback (2022)
** NLP中在PTMs思想下的一般性的神经网络架构
Pre-trained Models for Natural Language Processing: A Survey(2020)
See [[./images/NLP/Embedding.jpg]]
应当说,这篇文章把PTM总结得比较好!
** Prompting的花样年华
Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(2021)
四个NLP的范式,See: [[./images/NLP/prompting.jpeg]]
这篇综述,让Prompting红得发紫.其实这篇文章,应当是个典范.一片综述,让一个模糊概念有了清楚的正式定义,并指明了这个领域的研究范畴.
形式化地讲,在Prompt范式下,需要通过以下三个步骤建立从输入到输出的PipeLine.
- Prompt addition:
- Answer search:
- Answer mapping:
由是而有下面五方面的工作.
- Pre-trained Model Choice
- Prompt Engineering
- Answer Engineering
- Expanding the Paradigm
- Prompt-Based Training Strategies
** Prompting 可能意味一些更深刻的思想内涵!
!!!怎么说呢?

* Deformable Convolution 和 Transformer的一个局部较量!
商汤的2023 - InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions.
看来这篇文章将DCN推到一个可理论探索的地步!
