#+STARTUP:ident
#+TITLE: Generative Adversarial NetWork
#+AUTHOR: Reader

* Gan
- Loss
  + Gan
    - DCAGAN : 它提出了一种生成器和判别器的架构，这个架构能极大地稳定GAN的训练，以至于它在相当长的一段时间内都成为了GAN的标准架构。
      [[file:images/dcgan_architecture.png]]
    - ResNet : 目前GAN上主流的生成器和判别器架构确实已经变成了ResNet。出自那篇文章呢？
      [[file:images/resnet_architecture.png]]
  + Self-Mod : 这牵涉到一个实质性的话题。BN/cBN/Self-Mod/AdaIn的内在联系！
    - cBN：即BN中所谓的center/beta、scale/gamma依赖某条件。
    - Self-Mod: 将那个条件设定为z~noise.
    - AdaIn: 
  + WGAN :
    用Wasserstein Distance替代Jesson-Shannon Distance.考虑了D网的Lipschitz条件,但没实现.
    为进一步实现Lipschitz条件正则项，而出现下述改进
    - WGAN-GP : Gradient Penality
      进一步思考，选择什么样的样本来做梯度惩罚
      + WGAN-LP : Local Lipschitz Penality
      + DRAGAN :  Local Gradient Penality
  + SNGAN : Spectral Normalization
    出发点：线性函数的Lipschitz范数和矩阵的普范数有关系,故等等!
    本想将之划归WGAN下的，但很奇怪的是，它的Loss还是最标准的Gan-Loss,即JS-Distance(Jesson-Shannon Distance)家族.
  + LSGAN : Loss Sensitive GAN
  + LSGAN : Least Square Gan
- D的Weights
  + Weights-Clipping
  + Gradient Penality
  + Spectral-Normalization
* GAN的四个层面
** Level 0: Definition of GANs
[[./images/ganmodule.png]]
** Level 1: Improvements of GANs training
** Level 2: Implementation skill
** Level 3: GANs Applications in CV

