#+STARTUP: indent
#+TITLE: AGI_Application 有关一些AGI应用的笔记
#+AUTHOR: Reader

* RAG
- [2023 - Large Language Models on Graphs: A Comprehensive Survey]
   这篇文章修改了一年!2024年底还有修改!还是可以看看的。
   有人还为这篇文章写了博客!评论道，。。。，知识图谱(KG)的发展趋势已经从早期的HippoRAG,GraphRAG通过知识图谱增强LLM准确性、减少幻觉转变为更加垂直化的应用，
   聚焦于那些本来就适合知识图谱的行业，例如交通运动图、情报人物关系图谱、生物分子图和特定金融分析领域。
   ？问题:那些是适合知识图谱的行业？
* 基于LLM的Agent's Planning！
一般认为Autonomous agent具有完成三方面任务的能力:Perceiving,Planning,Excuting.
而Planning作为Agents的最关键的能力，是一个由理解(Understanding)、推理(Reasonging)、决策(Decision-Making)等过程交织在一起的进程!
目前对LLM Based Agents的Planning Capability的研究,...(其实,对基于LLM的Agents的各方面研究,都很热闹,但能否达成期望的目标,似乎没人在意!)
** [2025 - Understanding the planning of LLM agents: A Survey]
文章的贡献在于对LLM Based Agents' Planning的研究工作作了分类,让我们了解这个坑是怎么回事?:
- Task Decomposition,
  代表工作:CoT [2022],ReAct [2022],HuggingGPT [2023]
- Multi-plan Selection,
  代表工作:ToT [2023],GoT[2023],CoT-SC[2022]
- External Planner-Aided Planning,
  代表工作:LLM+P [2023],LLM+PDDL[2023]
- Reflection and Refinement,
  代表工作:Reflexion [2023],CRITIC[2023],Self-Refine[2023]
- Memory Augmented Planning,
  代表工作:REMEMBER [2023],MemoryBank [2023]
注:这一块的工作,不论是灌水还是投资,目前还是非常活跃!
* RAG
** Agentic Rag代表了什么进展？[2025 - Agentic Retrieval-Augmented Generation - A Survey on Agentic RAG - v1]
RAG->GraphRAG->KG?RAG
** LLM之RAG最新进展 - 边思考边搜索的迭代式生成
* OpenAI的ChatGPT o1带来了什么? DeepSeek又带来了什么?
OpenAI发布了ChatGPT o1!其出色的推理(Reasoning)能力,让学术界在猜测,它是否“内置”了思维链!怎么“重现”?
- UCL的汪军,"A tuturial on LLM Reasoning: Relevant methods behind ChatGPT o1"
- Fudan的,"Scaling of Search and Learning - A Roadmap to Reproduce o1 from Reinforcement Learning Perspective"
讲的,都是这方面的事!感觉汪军把问题说得较清楚!
一个新的方向:LLM原生思维链(LLM-Native Chain-Of-Thought/ Native-COT).这一步可能是迈向自我改进智能体(Self-Improving Agent)的重要一步!
感觉-知觉-思维的话题被显示提及!
传统自回归LLM面临的挑战:如何使系统超越其训练数据的界限并开发出新颖的可能更优的策略?
汪军作了如下的推测:
- 通过将“LLM的预测能力”与“强化学习”和“世界建模的策略”深度相结合,像o1这样的AI系统可以解决更复杂的问题和实现更复杂的决策过程.
- 在推理和解码阶段实现类似于蒙特卡洛树搜索(MCTS)的基于模型的复杂策略。
*** DeepSeek又带来了什么？
Scaling Law 的发现成为了LLM研究的大部分最新进展的催化剂. -- [2020 - Scaling laws for autoregressive generative modeling]
-- 可以看看[陈巍：DeepSeek V3/R1的架构与训练技术2万字长文分析(上,下)].
**** Scaling Law 之生
**** Scaling Law 之死
*** Test-time compute 到 Test-time scaling,难道仅仅如此？
