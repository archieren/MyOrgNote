#+STARTUP:ident
#+TITLE: Generative Adversarial NetWork
#+AUTHOR: Reader

* Gan LandScape
2018 - The GAN Landscape:Losses, Architectures, Regularization,and Normalization
2019 - A Large-Scale Study on Regularization and Normalization in GANs
** Architecture Base: Generator and Discriminator Architecture
  - DCAGAN : 它提出了一种生成器和判别器的架构，这个架构能极大地稳定GAN的训练，以至于它在相当长的一段时间内都成为了GAN的标准架构。
    - [[file:images/dcgan_architecture.png]]
  - ResNet : 目前GAN上主流的生成器和判别器架构确实已经变成了ResNet。出自那篇文章呢？未有明确的参考文章。
    - [[file:images/resnet_architecture.png]]
** Regularization and Normalization
  - Discriminator Normalization
    - 深度学习中，ICS(Internal Covariance Shift)问题，引发了BN/LN/WN等研究及运用。
      - 常规，暂略。
    - 在GAN领域，进而因Wasserstein Loss的原因，要求Discriminator具有Lipschitz continuity性质，引发了GP、SN的研究。
      - GP(Gradient Penality): 2017 - Improved Training of Wasserstein GANs
      - SN(Spectral Normalization): 2018 - Spectral normalization for generative adversarial networks
  - Generator Normalization
    - Generator中的研究，重在先验信息、条件信息的引入方法。这导至Gan的研究，繁荣面貌出现在Generator上。
    - cBN: 2018 - cGAN with projection discriminator
    - AdaIn: 2017 - Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization
    - Self-Mod: 2018 - On Self Modulation For Generative Adversarial NetWorks
      - Self-Mod考虑到cGAN训练的稳定性更好，但是一般情况下GAN并没有标签c可用，干脆以噪声z自身为条件标签好了！
      - BN、cBN、AdaIn、Self-Mod具有统一的形式，所以要看到这背后的统一理论蕴涵。
      - StyleGan可以看成Self-Mod的变种。StyleGAN是必须关注的。
** GAN's Objective: How to design GANs
  - GAN's objective.
    这里更多的是理论成果。对抗的目的，是为了得到复合某种期望的一个生成分布。
    + See "2016 - f-GAN - Training Generative Neural Samplers using Variational Divergence Minimization"
    + See "2020 - Designing GANs - A Likelihood Ratio Approach"
  - WGAN :
    用Wasserstein Distance替代Jesson-Shannon Distance.考虑了D网的Lipschitz条件,但没实现.
    为进一步实现Lipschitz条件正则项，而出现下述改进
    - WGAN-GP : Gradient Penality
      进一步思考，选择什么样的样本来做梯度惩罚
      + WGAN-LP : Local Lipschitz Penality
      + DRAGAN :  Local Gradient Penality
  - LSGAN : Loss Sensitive GAN
  - LSGAN : Least Square Gan
* GAN的四个层面
** Level 0: Definition of GANs
[[./images/ganmodule.png]]
** Level 1: Improvements of GANs training
** Level 2: Implementation skill
** Level 3: GANs Applications in CV
* 基于AnoGan思路的异常检测 - 这可能是有方向性误导的坑
** 2017 AnoGAN: Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery
[[./images/gan_anomaly/AnoGan.png]]
这篇论文是始作用者。
基本论点：用正常的数据训练出的GAN，针对异常者，在隐空间难有隐变量能很好地生成异常者。
         缺点：推理时，采用迭代的方法，找到输入的“最佳”隐变量。
** 2018-02 Efficient Gan-Based Anomaly Detection
[[./images/gan_anomaly/EfficientAnomalyGan.png]]
基于BiGAN，解决AnoGAN推理阶段的迭代方法的低效性问题。
** 2018-12 Adversarially Learned Anomaly Detection
[[./images/gan_anomaly/ALAD.jpeg]]
基于BiGAN，是上文的加强版。
** 2018-11 GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training
[[./images/gan_anomaly/GANomaly.png]]
重构放到核心位子。
** 2019-01 Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection
[[./images/gan_anomaly/SkipGANomaly.png]]
重构更精确。
** 2018 A Surface Defect Detection Method Based on Positive Samples
[[./images/gan_anomaly/PositiveSamplesGANomaly.png]]
人工缺陷样本生成。采用LBP进行输入与重构的逐像素比较。LBP意味着往知觉意义上进行比较。
** 2018 Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders
[[./images/gan_anomaly/CAAE.jpeg]]
讨论图像空间、隐空间的拓扑结构对距离的影响。
在这篇论文中，基本接近对AnoGAN这个思路的反思了。

* Deep Generative Models
Generative Models + Deep Neural Network = Deep Generative Models
生成模型是统计学里面的概念。
放在DGM概念下,前期研究得较多的,如:
VAE(Variational AutoEncoder),ARM(AutoRegressive Models),GAN(Generative Adversarial Network),EBM(Energy Based Models),FBM(Flow Based Models),等等.
但近来,SBGM(Score Based Generative Model), SDE(Stochastic Differential Equation), DDBM(Diffusion Probabilistic Models)似乎比较热闹.
而且它们三者有着深刻得联系.
** DM(Diffusion Model): SBGM & SDE & DBM......


