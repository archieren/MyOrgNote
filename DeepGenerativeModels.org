#+STARTUP:ident
#+TITLE: Generative Adversarial NetWork
#+AUTHOR: Reader

* Gan LandScape
2018 - The GAN Landscape:Losses, Architectures, Regularization,and Normalization
2019 - A Large-Scale Study on Regularization and Normalization in GANs
** Architecture Base: Generator and Discriminator Architecture
  - DCAGAN : 它提出了一种生成器和判别器的架构，这个架构能极大地稳定GAN的训练，以至于它在相当长的一段时间内都成为了GAN的标准架构。
    - [[file:images/dcgan_architecture.png]]
  - ResNet : 目前GAN上主流的生成器和判别器架构确实已经变成了ResNet。出自那篇文章呢？未有明确的参考文章。
    - [[file:images/resnet_architecture.png]]
** Regularization and Normalization
  - Discriminator Normalization
    - 深度学习中，ICS(Internal Covariance Shift)问题，引发了BN/LN/WN等研究及运用。
      - 常规，暂略。
    - 在GAN领域，进而因Wasserstein Loss的原因，要求Discriminator具有Lipschitz continuity性质，引发了GP、SN的研究。
      - GP(Gradient Penality): 2017 - Improved Training of Wasserstein GANs
      - SN(Spectral Normalization): 2018 - Spectral normalization for generative adversarial networks
  - Generator Normalization
    - Generator中的研究，重在先验信息、条件信息的引入方法。这导至Gan的研究，繁荣面貌出现在Generator上。
    - cBN: 2018 - cGAN with projection discriminator
    - AdaIn: 2017 - Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization
    - Self-Mod: 2018 - On Self Modulation For Generative Adversarial NetWorks
      - Self-Mod考虑到cGAN训练的稳定性更好，但是一般情况下GAN并没有标签c可用，干脆以噪声z自身为条件标签好了！
      - BN、cBN、AdaIn、Self-Mod具有统一的形式，所以要看到这背后的统一理论蕴涵。
      - StyleGan可以看成Self-Mod的变种。StyleGAN是必须关注的。
** GAN's Objective: How to design GANs
  - GAN's objective.
    这里更多的是理论成果。对抗的目的，是为了得到复合某种期望的一个生成分布。
    + See "2016 - f-GAN - Training Generative Neural Samplers using Variational Divergence Minimization"
    + See "2020 - Designing GANs - A Likelihood Ratio Approach"
  - WGAN :
    用Wasserstein Distance替代Jesson-Shannon Distance.考虑了D网的Lipschitz条件,但没实现.
    为进一步实现Lipschitz条件正则项，而出现下述改进
    - WGAN-GP : Gradient Penality
      进一步思考，选择什么样的样本来做梯度惩罚
      + WGAN-LP : Local Lipschitz Penality
      + DRAGAN :  Local Gradient Penality
  - LSGAN : Loss Sensitive GAN
  - LSGAN : Least Square Gan
* GAN的四个层面
** Level 0: Definition of GANs
[[./images/ganmodule.png]]
** Level 1: Improvements of GANs training
** Level 2: Implementation skill
** Level 3: GANs Applications in CV
* 基于AnoGan思路的异常检测 - 这可能是有方向性误导的坑
** 2017 AnoGAN: Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery
[[./images/gan_anomaly/AnoGan.png]]
这篇论文是始作用者。
基本论点：用正常的数据训练出的GAN，针对异常者，在隐空间难有隐变量能很好地生成异常者。
         缺点：推理时，采用迭代的方法，找到输入的“最佳”隐变量。
** 2018-02 Efficient Gan-Based Anomaly Detection
[[./images/gan_anomaly/EfficientAnomalyGan.png]]
基于BiGAN，解决AnoGAN推理阶段的迭代方法的低效性问题。
** 2018-12 Adversarially Learned Anomaly Detection
[[./images/gan_anomaly/ALAD.jpeg]]
基于BiGAN，是上文的加强版。
** 2018-11 GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training
[[./images/gan_anomaly/GANomaly.png]]
重构放到核心位子。
** 2019-01 Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection
[[./images/gan_anomaly/SkipGANomaly.png]]
重构更精确。
** 2018 A Surface Defect Detection Method Based on Positive Samples
[[./images/gan_anomaly/PositiveSamplesGANomaly.png]]
人工缺陷样本生成。采用LBP进行输入与重构的逐像素比较。LBP意味着往知觉意义上进行比较。
** 2018 Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders
[[./images/gan_anomaly/CAAE.jpeg]]
讨论图像空间、隐空间的拓扑结构对距离的影响。
在这篇论文中，基本接近对AnoGAN这个思路的反思了。

* Deep Generative Models
Generative Models + Deep Neural Network = Deep Generative Models
生成模型是统计学里面的概念。
放在DGM概念下,前期研究得较多的,如:
VAE(Variational AutoEncoder),ARM(AutoRegressive Models),GAN(Generative Adversarial Network),EBM(Energy Based Models),FBM(Flow Based Models),等等.
但近来,SBGM(Score Based Generative Model), SDE(Stochastic Differential Equation), DDPM(Denoising Diffusion Peobability Model)似乎比较热闹.而且它们三者有着深刻得联系.
同时,FM(Flow Matching)和DM(Diffusion Models)被认为是一个硬币的两个方面！(See https://diffusionflow.github.io)

** DM(Diffusion Model): SBGM & SDE & DBM......
还是cs236课程里,Stefano Ermon讲得比较清晰!
** Score Function充当的角色?
[Hyvärinen A.]2005 - Estimation of Non-Normalized Statistical Models By Score Matching
[Vincent P.]2011 - A Connection Between Score Matching and Denoising Autoencoders
[Welling M.;...]2011 - Bayesian Learning via Stochastic Gradient Langevin Dynamics
** FM(Flow Matching):
[Kingma D.P.;Gao R.Q]2023 -  Understanding diffusion objectives as the elbo with simple data augmentation -v7
[Albergo M.S.;...]2023 - Stochastic Interpolants - A Unifying Framework for Flows and Diffusions - v3
[......]
[Lipman Y.]2022 - FM - Flow Matching for Generative Modeling - v2
[Liu Q.]2022 - Rectified flow - a marginal preserving approach to optimal transport

概念上要区分一下:
- FBM(Flow-based Model)是一种基于Normalizing Flows(NFs)的生成模型,它通过一系列概率密度函数的变量变换,将复杂的概率分布转换为简单的概率分布,并通过逆变换生成新的数据样本.
- CNFs(Continuous Normalizing Flows)是Normalizing Flows的扩展,其概念首次出现在Neural ODE中!它使用常微分方程(ODE)来表示连续的变换过程,用于建模概率分布.
- FM(Flow Matching)是一种训练CNFs(Continuous Normalizing Flows)的方法,它通过学习与概率路径相关的向量场(Vector Field)来训练模型，并使用ODE求解器来生成新样本.
- DM(Diffusion Model)是FM(Flow Matching)的一个应用特例,使用FM可以提高其训练的稳定性.
- 此外,使用最优传输(Optimal Transport)技术构建概率路径可以进一步加快训练速度.并提高模型的泛化能力.
- 似乎可以某种观点将他们统一!

彻底思考:
- Continuty Equation/Transport Equation 是否在底下支撑什么？
** 不算是题外话.很多模型的起点,似乎都可以追述到对VAE的认知!为什么?
