#+STARTUP: indent
#+TITLE: NeuralNetwork 阅读纪要

* 理部
** 概要
对于这些年的盲目阅读，很难给出一个合乎逻辑的条理来。
但一直以来，围绕着Retinotopy（Tonotopy 从没关注过）的Functional Architecture, Computational Model, Biology Origin等，隐含地关注着一下几个主题方面：
- Retina-LGN的感受野的非线性结构。以Saccade/Microsaccade为线索，引出来的一系列问题！
- Retina-LGN-Cortex究竟在何处已经有视觉功能,这是迷一样的问题.目前这一块的研究,越来越认为,在Retina,就有了视觉!!!
- LGN-Cortex的稀疏问题。LGN供给Coretex的输入是非常稀疏的，却能产生极其复杂的。。。！这意味着什么？
- The Functional Architecture of The Visual Cortex的形成机理？和Orientation Maps的起源！
- Cortex的Column/Microcolumn结构的一般化问题。
- 在Perceptual Grouping问题下，思考association问题。对NeuralGeometry这个小众的数学模型很着迷。

*** Retina:Microsaccade and saccade.
1. [Field G.D. ;Chichilnisky E.J. ]2007-Information Processing in the Primate Retina
   总结了视网膜在神经生理学、解剖学上的新认知。
   - Many distinct pathways of visual information emanate from the retina.
     - 最早开端
       猫视网膜上的RGC(Retina ganglion cell)研究。
       生理学上发现了X(linear)/Y(nolinear) cell。解剖学上发现了alpha/beta cell。
       由是而有Two PathWay的研究主线。
       如，在灵长类的研究中，Midget（侏儒）/Parasol（伞状）cell投射到LGN的Parvocellular（小细胞的）/Magocellular（大细胞的）层，继而投射到视觉新皮层。
       这两条路径，占据了各项研究的主角。
     - Small Bistratified RGC的发现。
       Small Bistratified Cell投射到LGN的Koniocellular（粒状细胞的）层，继而投射到视觉新皮层的路径又多了一条。
       至此，这三条路径，构成了目前教科书的主要内容。太简化问题了！
     - 但近来，还发现更多的新RGC类型,及更多的投射路径。
       按形态来划分，并合并On/Off子类，如：
       - Momostratified: Midget/Parasol/Sparse/GiantSparse/Recursive/Smooth/BroadThomy/Thomy
       - Bistratified: Recursive/Large/Small
       等RGC类型，共产生了11条到LGN的路径。
       此外，还有些RGC，产生了到Superior colliculus（上丘）及其他Nonthalamic structure的投射路径。
       许多RGC还会投射到至少两个中央结构的投射路径。
     - 不寻常的视觉信号（Novel visual signals）
       令人惊讶的是，这些新发现的RGC的生理特性并不是很清楚。
     - 视觉路径的多样性，发端于视网膜内。（Intra-retina origins of diversive visual pathways）
       =在视网膜内的线路中解释多样性。（Interpretation of diversity in rentina circuits）=
     - 影响（Implication）：
       对视网膜的研究：
       对视觉系统的研究：The visual signals emanating from multiple distinct RGC types may remain segregated in the cortex!也就是有多种信号进入新皮层!
       对大脑的研究：脑皮层里Cell-Type diversity可能有待大发现!

   - Precesion OF Retinal Spike Trains And Models Of The Neural Code.
     Nonlinear spatial-temporal integration encoding成了新认识！
     Many common biophysical phenomena, such as refractoriness, bursting, spike frequency adaptation, and oscillation, give rise to intrinsic temporal structure in spike trains.
     Thus,to understand how neural circuits process and represent information generally requires measurements and models that *go beyond descriptions of firing rate*.

   - Synchronized firing and consequences for visual signaling
     The significance of synchronized firing and other patterns of multineuron activity is a fundamental problem in many neural circuits.
     The structure of synchronized firing in RGCs —regularity, spatial locality, and cell-type specificity— may provide clues about other neural structures.
   - Decoding the visual signal from retina spike trains

   - Conclusions
     - At the photoreceptor synapse, multiple distinct visual signals begin to emerge.
     - These signals are processed by a diverse collection of bipolar and amacrine cell types that converge on at least 17 distinct types of RGCs, each of which covers the visual field with striking regularity.
2. [Rolfs M.]2009 - Microsaccade - Small steps on a long way
   [Rucci M.;Ahissar E.;Burr D.]2018 - Temporal Coding of Visual Space
   Mircrosaccade对visual signaling究竟有什么本质的影响？
3. 可以细读[Gollish T.;...]2010 - Eye Smarter Than Scientist Believed。
4. [Schwartz G.W.]2012 - The spatial structure of a nonlinear receptive field
   =Two features= of spatial integration in RGCs presents major obstacles for a predictive model: nolinearity and fine-scale heterogeneity.
   =Model based on anatomical and physiological mechanisms=
   Their *anatomically* based model captured the response profiles of individual RGCs more closely than previous receptive-field models based on linear spatial integration or a Gaussian weighting of nonlinear subunits.
   附注：Schwartz G.W.的后期工作，尽管不是retina方面的，也还值得关注。他基于图论，一直在作Anatomically based model的工作。
   Fred Rieke就是写Spike那本书的作者，其实验室的工作更值得关注。
   1. [Schwartz G.W.]2011 - Nonlinear spatial encoding by retinal ganglion cells_ when 1 + 1 ≠ 2
5. [Gollisch T.;...]2013 - Features and functions of nonlinear spatial integration by retinal ganglion cells
   概述了鉴别Nonlinear signal integration的各类方法、途径。
   最后回到他们自己的ISO-Reponse方法。
   不需要太深的解剖细节，就能评价非线性集成的方式!
   4和5两篇，对subfields/subunits LN的推动比较大。
   *Iso-response measurements represent an alternative, as they provide a way to assess nonlinear stimulus integration without the need of an a priori parameterization of the nonlinearities.*
6. [Martinez-Conde S.]2013 - NRN - The impact of microsaccades on vision
7. [Real E.;Asari H.;Golish T.;Meister M.]2016 - Neural Circuit Inference from Function to Structure
   有趣的话题，基于模型，将Circuits推断出来！
8. Tuturial - Statistical Models for Neural Data: from Regression / GLMs to Latent Variables - Pillow J.W. - 2016
   [Pillow J.W.]2008 - Spatio-temporal correlation and vision signalling in a complete neuronal population
   事实上，从2008这篇文章开始，Pillow的工作，就集中在 *How does the spiking activity of a neural population represent the sensory environment?*
9. [Shah N.P.;Simoncelli E.;Chichilnisky E.J.]2020 - Inference of nonlinear receptive field subunits with spike-triggered clustering
   [Liu J.K.;...]2017 - Inference of neuronal functional circuitry with spike-triggered non-negative matrix factorization
   [Freeman J.;....]2014 - Mapping nonlinear receptive field structure in primate retina at single cone resolution
   [Field G.D.;Chichilnisky E.J.]2010 - Functional connectivity in the retina at the resolution of photoreceptors
   ......
   这其实Simoncelli E. Chichilnisky E.J.宗派的系列文章。
信号的多样性、信号激励的非线性整合的多样性，是这些故事的内容。
视网膜的故事,最终引向这句话:Subunits appear to be a common computational motif in the brain.
但带来的问题: Encoding and decoding in a heterogeneous population!
*** Visual Neocortex
** Place Cells/Grid Cells/Head Cells/Border Cells/Speed Cells/Trace Cells/Object Cells/Object vector Cells, and so on
观看视频"Grid Cells, Object Representations, and Memory" & "Grid Cells and the Brain's Spatial Mapping System"
Memory is a process reproduce/reconstruction!? (David Marr)
Navigation is the core!? Spatial Mapping --abstract--> Cognitive Map !?
*** Brain Regions Involved in Spatial Cognition
涉及空间认知的脑区域,下图示意.("Roddy M Grieves and Kate J Jeffery. The representation of space in the brain. Behavioural Processes, 135:113–131, 2017.")
[[file:./images/GridCells/SpatialRecognition.jpg]]
*** 网格细胞的基本性质
**** Oritention, WaveLength, Phase
确切地说,其感受野的特性:具有格子性状!
[[file:./images/GridCells/Attributes.jpg]]
**** Changing as Enviroment Changing
- 网格细胞锚定于外界的地标
  网格细胞会锚定于外界的感觉地标，当地标旋转时，网格细胞的网格结构也会跟着旋转。
  [[file:./images/GridCells/ChangeWithAnchor.jpg]]
- 网格细胞的网格会被外界空间压缩和拉伸
  当改变大鼠所处笼子的尺寸时，网格细胞的网格会发生相应的改变。
  外界空间变大时，网格细胞的网格被扩展，间距变大。
  相反，外界空间变小时，网格细胞的网格会被压缩，间距变小。
  [[file:./images/GridCells/ChangeWithScale.jpg]]
- 网格细胞的网格会受到来自空间边界的切力，继而重排列。
  在一个方形的笼子里，大鼠网格细胞发放野的朝向并不是垂直于笼子边界的。
  这是因为笼子的边界墙壁会对网格细胞的网格产生一种切力，使网格轴发生一定角度的偏转。
  [[file:./images/GridCells/ChangeWithBoundary.jpg]]

  切力（Shear force）的轴为空间边界，切力的方向沿着空间边界这条轴，切力的大小随着离边界轴距离的增加而衰减。
  因此，距离边界越近的位置，切变形越严重。
  [[file:./images/GridCells/ChangeWithBoundary_I.jpg]]
- 当视觉参照物位置不变，但空间旋转时，网格细胞的网格也会重排列。
  [[file:./images/GridCells/ChangeWithSpaceRotation.jpg]]
- 网格细胞的网格会随着对空间的熟悉程度而发生变化
  空间边界的切力会让网格轴向偏移一定角度。
  但科学家发现，当大鼠刚进入一个陌生空间时，网格细胞的网格轴并没有偏移，而是垂直边界。
  等到大鼠完全熟悉该空间以后，偏移才会发生。
  网格细胞在一个空间内的放电模式，是对该空间适应的结果。
  [[file:./images/GridCells/ChangeWithFamilarity.jpg]]
  大鼠身处两个连通的隔间之中。
  最开始，大鼠并不熟悉两个房间的空间位置结构，此时每个网格细胞会为每个房间生成一张独立的“网格地图”。
  待大鼠完全熟悉这两个空间后，两个独立的网格结构会融合成一体。
  [[file:./images/GridCells/ChangeWithFamilarity_I.jpg]]
**** Visual Grid Cells在人脑EC区域中的发现
在[Nathaniel J.K.; Elizabeth A.B.]2018 - Grid cells map the visual world一文中，
对2018年的发现:
  - [Julian J.B.;Keinath A.T.;Frazzetta G.;Epstein R.A.]2018 - Human entorhinal cortex represents visual space using a boundary-anchored grid
  - [Nau M.;Schröder T.N.;Bellmund J.L.S.;Doeller C.F.]2018 - Hexadirectional coding of visual space in human entorhinal cortex
作了一个概述和总结。同时也坐实了他们在[Nathaniel J.K.;Michael J.J.;Elizabeth A.B.]2012 - A map of visual space in the primate entorhinal cortex中的猜想.
Visual Exploration, Locomotion等广泛的生物行为都能对空间采样及编码,这意味着类似Grid Cell这样的结构,具有更基础的认知功能!
*** Grid Cells的计算模型,形成理论,认知理论,等等!

**** [Stensola T.;Moser E.I.]2016 - Grid Cells and Spatial Maps in Entorhinal Cortex and Hippocampus
It's a review!
***** Place Cells and Grid Cells神经生理发现
- Place Cells:  ... Tolmanian cognitive maps
- Grid Cells: ...
***** Grid-to-Place Transformation中的神经生理
Grid Map的组织方式: 沿着 MEC 的 DorsalVentral Axis 的 Modular Organization. See "Discretization of the Entorhinal Grid Map"
***** Computational Models Of Grid-to-Place Transformation
一堆的猜想，留待慢慢阅读吧？
**** Sorscher B.的工作
- [Sorscher B.]2019 - A unified theory for the origin of grid cells through the lens of pattern formations
- [Sorscher B.]2022 - A unified theory for the computational and mechanistic origins of grid cells
这两篇文章,其实是一篇,可能会是一片重要的文章! - 最终,2022年,修改了一下后,改名,在Neuron期刊上发表!
......
~"我们现在超越网格细胞的计算起源,转向理解这些细胞在训练神经网络中的机制起源."~
[注:而[Dumont N.S.Y.;...]2020 - Accurate representation for spatial cognition using grid cells 一文中,也据此来论证SSP的生物合理性.]
......
说来说去,两个核心贡献:
- Pattern formation theory predicts structure of learned representation
- Non-negativity Constraints

**** Whittington J.C.R这一脉的工作: ?!Disentanglement
- [...;Whittington J.C.R]2022 - Actionable Neural Representations - Grid Cells from Minimal Constraints
- [Whittington J.C.R.;...]2023 - Disentanglement with Biological Constraints - A Theory of Functional Cell Types
这是两篇相伴的文章,内在的思想是一致的,只不过应用的领域不同!
We formalise these ideas - ~actionable~, ~functional~, and ~biological~.
Biological representations must be more than just actionable - they must be functional, encoding the world efficiently, and obey biological constraints.
- ~Actionability~, using group and representation theory, we define it as the requirement that each action has a corresponding matrix that linearly updates the representation;
- ~Functionally~, we want different points in space to be represented maximally differently, allowing inputs to be distinguished from one another.
- ~Biologically~, we ensure all neurons have non-negative and bounded activity.
**** Schaefer R. ... Fiete I.R. 这一脉的工作！
~[ΔΔ;Fiete I.R.]2024 - Bridging Associative Memory and Probabilistic Modeling~
[Schaefer R.;...;Fiete I.R.]2023 - Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells
**** 基于Conformal Isometry Hypothesis的工作！
[2025 - On Conformal Isometry of Grid Cells - Learning Distance-Preserving Position Embedding]

** New approaches to Deep Networks
这是一篇评论，议及四种网络
- CapsuleNet  (Hinton  @ ??)
- HTM         (Hawkins @ Numenta)
- Sparsey     (Rinkus  @ Neurithmic Systems)
- RCN         (George  @ Vicarious)

References
- Sabour, S., Frosst, N. & Hinton, G.            Dynamic Routing between Capsules. (2017).
- Hawkins, J., Ahmad, S. & Cui, Y.               Why Does the Neocortex Have Layers and Columns, A Theory of Learning the 3D Structure of the World.(2017).
- George, D. & Hawkins, J.                       A hierarchical Bayesian model of invariant pattern recognition in the visual cortex.(2005).
- Hawkins, J. & George, D.                       Hierarchical temporal memory: Concepts, theory and terminology.(2006).
- George, D. & Hawkins, J.                       Towards a mathematical theory of cortical micro-circuits.(2009).
- Hawkins, J., Ahmad, S. & Dubinsky, D.          HTM Cortical Learning Algorithms.(2011).
- Hawkins, J. & Ahmad, S.                        Why neurons have thousands of synapses, a theory of sequence memory in neocortex.(2016).
- George, D. et al.                              A Generative Vision Model that Trains with High Data Efficiency and breaks text-based CAPTCHAs.(2017).
- Rinkus, G. J.                                  A cortical sparse distributed coding model linking mini- and macrocolumn-scale functionality.(2010).
- Rinkus, R. & Leveille, J.                      Superposed Episodic and Semantic Memory via Sparse Distributed Representation. (2017).

** Sparsey - Event Recognition Via Deep Hierarchical Sparse Distributed Code - 2014
注：在《Radically New Theory of how the Brain Represents and Computes with Probabilities》里又总结了他的那些激进理论。
A macro/mini-column model of cortical computation
cells -> mini-columns (competitive modulars) -> macro-columns

作者反思群编码的问题：
Most previous probabilistic population coding (PPC) theories share basic properties:
1) continuous-valued units;
2) fully/densely-distributed codes;
3) graded synapses;
4) rate coding;
5) units have innate unimodal tuning functions (TFs);
6) units are intrinsically noisy;
7) noise/correlation is generally considered harmful.

They present a radically different theory that assumes:
1) binary units;
2) only a small subset of units, i.e., a sparse distributed representation (SDR) (a.k.a. cell assembly, ensemble),comprises any individual code;
3) functionally binary synapses;
4) signaling formally requires only single(i.e., first) spikes;
5) units initially have completely flat TFs (all weights zero);
6) units are far less intrinsically noisy than traditionally thought;
7) rather noise is
   - a resource generated/used to cause similar inputs to map to similar codes,
   - controlling a tradeoff between storage capacity and embedding the input space statistics in the pattern of intersections over stored codes,
   - epiphenomenally determining correlation patterns across neurons.

** RCN - Recursive Cortical NetWork
RCN integrates and builds upon various ideas from Compostional Models - ... - into a structured probabilistic graphical model such that Belief-Propagation can be used as the primary approximate inference engine.
** Capsulenet
对这个网络,不能用纯粹的数学或技术眼光看.要看他们二十年来在坚持什么！它那背后简单的哲学！
*** 从"Transforming auto encoder"开始,提出Capsules这个比喻！"Capsules encapsulate all important information about the state of the feature they are detecting in vector form."
下面是那篇文章中最重要的话:
Instead of aiming for viewpoint invariance in the activities of “neurons”
that use a single scalar output to summarize the activities of a local pool of replicated feature detectors,

artificial neural networks should use local “capsules”
that perform some quite complicated internal computations on their inputs
and then encapsulate the results of these computations into a small vector of highly informative outputs.


Each capsule
learns to recognize an implicitly defined visual entity
over a limited domain of viewing conditions and deformations
and
it outputs
both
the probability that the entity is present within its limited domain
and
a set of “instantiation parameters”
that may include the precise pose, lighting and deformation of the visual entity relative to an implicitly defined canonical version of that entity.

When the capsule is working properly,
the probability of the visual entity being present is locally invariant
— it does not change as the entity moves over the manifold of possible appearances within the limited domain covered by the capsule.
The instantiation parameters, however, are “equivariant”
— as the viewing conditions change and the entity moves over the appearance manifold,
the instantiation parameters change by a corresponding amount because they are representing the intrinsic coordinates of the entity on the appearance manifold.

理解起来的话，就是:
"Capsules encapsulate all important information about the state of the feature they are detecting in vector form."
CNN丢失了feature的状态信息.
"胶囊"不仅要提供feature出现的概率,还要提供feature的状态信息！
!!但这样看来,和其他的"column"新理念,还是有差异的!


*** "Dynamic Routing Between Capsules"
这篇文章是"Capsules"理念的又一技术上的探索.
**** 技术一: "Routing by agreement"
Compositional Gramar, Hierarchical Representation, Parse Tree 等等,是脑内知识表示的比喻,这似乎是"所期望"的共识！但如何达成这样的表示呢？
动态路由要解决的问题是:自底向上,构造层次表示！ -- From bottom to up, to construct Parse Tree or Hierarchical Representation!
Lower level capsule will send its input to the higher level capsule that “agrees” with its input.
表面看来有点逻辑玄奥.
**** 技术二: "Squashing"
"We want the length of the output vector of a capsule to represent the probability that the entity represented by the capsule is present in the current input."
**** 技术三: 其实是个假设,"At each location in the image, there is at most one instance of the type of entity that a capsule represents."


** HTM(Hierarchical Temporal Memory)
(目前,有这种理论变化 HTM => The Thousand Brains Theory of Intelligence)
*** Biological and Machine Intelligence: - A digital book that documents Hierarchical Temporal Memory (HTM) - 这里面可能真有戏!
这是关于HTM的技术文档。
**** HTM Principles
神经生理学上的那些内容，启发了作者的基本原理？
- Biological Observation: =The Structure of the Neocortex=
  HTM principle:          =Common Algorithms [Cellunar layers - Mini-Columns - Columns]=
- Biological Observation: =Neurons are Sparsely Activated=
  HTM principle:          =Sparse Distributed Representations(SDR)=
- Biological Observation: =The Inputs and Outputs of the Neocortex=
  HTM principles:
  1. Sensory Encoders
  2. HTM Systems are embeded within sensory-motor Systems
  3. HTM relies on streaming data and sequence memory
  4. On-Line learning
**** Sparse Distributed Representations
- Capacity of SDRs and the probability of mismatches
- Robustness of SDRs and the probability of error with noise
- Reliable classification of a list of SDR vectors
- Unions of SDRs
- Robustness of unions in the presence of noise
需要注意的是,SDR应当是Distributed Vector Representation(DVR)这一庞大议题下的子议题.!
需要反思的是，“稀疏表示”路线有多少种。这个在"HTM Spatial Pooler"里总结一下！
**** Encoding Data for HTM Systems
**** Spatial Pooling algorithms
**** Tempory Memory Algorithms
**** Voting across columns
**** Location Layers in Grid Cells
*** 几篇论文：
**** Why neurons have thousands synapses - a Theory of Sequence Memory in Neocortex [Hawkins J]2015
建立了 HTM model neurons.
**** The HTM Spatial Pooler — A Neocortical Algorithm for Online Sparse Distributed Coding [YuWei Cui]2017
Sparse Coding技术中存在的问题：
- 着重表示，没搞清计算本质。
  重构误差最小+“某条优化原则: 能耗最小，稀疏表示反映客观世界的稀疏结构等等”
  Most previous studies propose the goal of sparse coding is to avoid information loss, reduce energy consumption, form associative memory with minimum cross talk, and so on.
  但问题是，它们没考虑Sparse Coding的计算特性。即稀疏码，适用于什么样的计算体系，如何用于计算，等等一系列问题。
- HTM Spatial Pooler中考虑的因素
  Sparseless, Entropy, Noise Robustness, Stablity.
**** 关于Location Based FrameWork的思想的形成，目前看三篇文章:
- =A Theory of How Columns in the Neocortex Enable Learning the Structure of the World [Hawkins J.]2017=
  Columns and Layers 是个通用结构,这意味着什么?
- =Locations in the Neocortex - A Theory of Sensorimotor Object Recognition Using Cortical Grid Cells [Lewis M.]2019=
- =A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex [Hawkins J.]2019=
  这篇文章提出了Location Based Framwork的思想.
** The Origins of Orientation Maps in V1
Neural tuning to visual stimulus orientation is one of the hall-marks of the primary visual cortex (V1) in mammals.
The orientation map is a hallmark of primary visual cortex in higher mammals.
*** Retina - Thalamus - Cortex
The Orientation map的形成机理,......
A key assumption of the current model is that neighboring V1 neurons receive feedforward inputs from a similar population of nearby RGCs, as suggested by the statistical wiring model.
见[Ringach D.]的2004，2007的文章。
但这个模型没有直接的实验数据来作支撑，故引出些许争议。
1. [Schottdorf M.;...]2015 - Random Wiring, Ganglion Cell Mosaics, and the Functional Architecture of the Visual Cortex
   这篇文章用实验作比较研究，值得看完。可了解对V1 functional architecture的形成机理进行解释的各种假说，以及它们之间的争论！
   但此文似乎是在用数据，质疑最新的Statistics wiring model（如Moire Interference Model），因为它还是不能满足‘common design’的试验结果。
   'Common Design'这一概念来至"[Kaschube M.;...]2010 - Universality in the evolution of orientation columns in the Visual Cortex"。
   有两种相对立的形成机理假设模型
   1) Statistics wiring model。 Moire Interference Model是目前较简洁有效的模型。但是受到此文的温柔质疑。
   2) Long-Range interaction model。 Kaschube M.用这类模型中的SOM来解释他发现的Universality，即所谓的‘common design’。
   此二者也代表了Forward和Recurent的争论。
   至于结论，我们细看。
2. [Paik SB.;Ringach D.]2011 - Retinal origin of orientation maps in the visual cortex
3. [Paik SB.;Ringach D.]2012 - Link between orientation and retinotopic maps in primary visual cortex
   此二文，给出了 Origin of orientation maps 的 Moire Interference机理猜想。这个值得注意。
   同时将Origin问题，延伸到Retina。Retina Mosaics 近乎可确定 Orientation Map。
4. [Markram H.]2015 - Reconstruction and Simulation of Neocortical Microcircuitry
5. [Jang J.]2020 - Retino-Cortical Mapping Ratio Predicts Columnar and Salt-and-Pepper Organization in Mammalian Visual Cortex
   实验验证了V1面积/R面积，这样一个参数，可以推断 V1 Orientation Map的类型，连续的还是椒盐的。
   讨论出这样的观点：
   1) Firstly, seeded by forward afferents.
   2) Secondly, fine-tuned by various types of synaptic plasticity in feedfor-ward and recurrent circuits
** Cognitive Map
*** Cognitive Map的概念
**** Tolman et al.在上世纪上半页, 在研究Rats在Maze中的行为时,提出的一个抽象概念.
"...referring to a rich internal model of the world that accounts for the relationships between events and predicts the consequences of actions."
这个概念,首先在认知神经科学的"spatial behavior"中,产生了重大影响.
引导了一系列的发现:
- Place cells (O’Keefe and Nadel, 1978)
- Grid cells (Hafting et al., 2005)
- Social place cells (Danjo et al., 2018; Omer et al., 2018)
- Head-direction cells (Taube et al., 1990)
- Object-vector cells (Høydal et al., 2018)
- Reward cells (Gauthier and Tank, 2018)
- Boundary vector cells (Lever et al., 2009)
- Goal direction cells (Sarel et al., 2017)
这些空间细胞似乎具有特定的功能表示,
但,容纳这些细胞的大脑组织结构,却同时,在generalization, inference, imagination, social cognition, and memory等神经处理任务中起着重要作用.
这些神经活动或处理任务, 是和广义的认知图有关的.
因此,这些类细胞在复杂的,高维的,非空间的认知图中,如何组织知识,是即将面临的挑战.
一个抽象的认知图,如何能数学地描述呢?这些描述或表示,又如何编码进网络中,进行运算?
*** Learning set
Harlow et al.在上世纪上半页,在研究" Discriminating a rewarding from two or more stimulus"学习行为时,
发现了一些有趣的现象,学习者可能学到了一些抽象知识,因此,他猜测有"a learning set"的机制存在, 用以表示抽象知识.
抽象知识的学习和表示,如何数学地描述?这一路线,我还没有基础知识面,待深入!
*** [Behrens T.E.J.;...]2018 - What Is a Cognitive Map
这篇文章提供了不错的概述!
Cognitive Map 和 Leaning Set又能怎样地发生关系呢?
** Semantic Pointer Architecture(SPA), Vector Symbolic Architecture and Spatial Semantic Pointer(SSP)
认知科学, 科学地研究诸如"attention, language use, memory, perception, problem solving, creativity, and thinking"等心智过程(mental process).
- “most cognitive scientists agree that knowledge in the human mind consists of mental representations”
- “cognitive science asserts: that people have mental procedures that operate by means of mental representations for the implementation of thinking and action"

有各种证据表明,有多类 Mental Representation(MR)存在.
可以看到,在Recognitive Model,或更大的目标, Brain Model中, Mental Representation 处于关键位置.
在神经网络的架构下,它需要满足:
- Implementable(? Encoderbale and Decoderbale).
  将信号,特征空间等,以神经编码的方式,得到心智表示. 反过来,有相应的过程!
- Transformable or Composable or Operatable.
  在心智表征上,施以各式心智处理过程.
- Structured Organization or Grouping.
  心智表征,是层次的,结构化的.
- Dynamics.

近来,我看到很多有关表示或表征的文章,各有巨大差异,但似乎均有共同的猜想出处, 即 Cognitive Map.而且,似乎都在想攻克"Spatial Behavior"这个具体的认知领域, 以论证自己的设想的现实性.

*** Neural Engineering FrameWork(NEF)
The Neural Engineering Framework (NEF) provides a set of principles for performing computations with spiking neural networks.
这是一套SNN的设计原则或方法学.
- Neural representations are defined by the combination of nonlinear encoding (exemplified by neuron tuning curves) and weighted linear decoding.
- Transformations of neural representations are functions of variables that are represented by neural populations.
  Transformations are determined using an alternately weighted linear decoding,..
- Neural dynamics are characterized by considering neural representations as control theoretic state variables.
  Thus, the dynamics of neurobiological systems can be analyzed using control theory.
*** SPA理论构想,在一本书"How to build a brain"系统地提出来.
概括说来, 语义指针(Semantic Pointer)假设认为:
- Higher-level cognitive functions in biological systems are made possible by semantic pointers.
- Semantic pointers are neural representations that carry partial semantic content.
- Semantic pointers are composable into the representational structures necessary to support complex cognition.

用术语语义指针,在于:
- 架构里的(语义)表示,有些类似计算机科学里的指针,能够获取大量的信息,而这些信息并未在语义表示中,直接承载或表示.
- 但它又是语义性的,因为这些表示的距离,如同联结主义设想的,能够扑捉语义矢量空间的关系.

Semantic Pointer Architecture (SPA; Eliasmith, 2013), which proposes a means of neurally implementing VSAs for explaining cognitive behaviour in biologically plausible spiking networks.
也可参考: [Gosmann Jan]2018 - PhD_Thesis -  An Integrated Model of Context, Short-Term, and Long-Term Memory
VSA是SPA的一个实例,易于在具有生物合理性的Spiking Neural Network中实现VSA,并可解释诸多认知行为!
*** Vector Symbolic Architecture (VSA)
Vector Symbolic Architecture(VSA)是离散表征的一支,在高维矢量空间建立某种代数结构,并用代数来编码这些离散的MR结构.
？Architecture指的是基于Vector Symbol构建Mental Representation的方法学
VSAs have been used to characterize a variety of cognitive behaviours,......
When VSAs are used to model cognitive behaviours, they essentially define methods for characterizing continuous vectors as both slots and fillers and define a method of binding fillers to slots.

VSA的关键基础在于那个代数结构如何定义.(参考: [Gosmann Jan]2018 - PhD_Thesis -  An Integrated Model of Context, Short-Term, and Long-Term Memory)
Three types of operators are considered essential in a VSA.
- A measure of similarity: s
- A superposition operator: S
- A binding operator(with an approximate inverse or unbinding operation ): B

*** Spatial Semantic Pointer
目前,前述内容知道大概就行.
- 令人奇怪的是, SPA中涌现出SSP,能够很好地描述Grid Cells的相关内容!
  三个层面: Vector Space + SPA + SSP
  - Vector Space(大家都知道)
  - VSA(Vector Symbolic Architecture)是SPA(Semantic Pointer Architecture)的一个特例!(看Gosmann J.的博士论文)
    矢量空间: Semantic Pointers构成了Vector Space!
    语义算子: 相似, 叠加, 绑定. 特殊元素: Identity vector, Absorbing Element, Unitary Vector
    重点研究或猜测这些代数对象怎么实现! 其中, Unitary Vector构成的流型空间, 值得研究一下.
  - 当绑定算子是循环卷积运算时, 可定义"fractional binding",由是有了空间语义指针SSP的概念!
    SSP 在 [Komer Berent]2019 - A neural representation of continuous space using fractional binding 一文中提出.
- SSP 在动力学方面的表现,值得关注!
  [Voelker A.R.]2021 - Simulating and Predicting Dynamical Systems With Spatial Semantic Pointers

** KAN,RPN,...还有谁？这是在思考什么？
*** 2024 - Kan: Kolmogorov-arnold networks
*** 2024 - KAN 2.0:Kolmogorov-Arnold Networks Meet Science
*** 2024 - RPN - Reconciled Polynomial Network Towards Unifying PGMs - Kernel SVMs - MLP - KAN
** World Models在讨论什么？
- 思想者,总是前行者.[[;Jürgen Schmidhuber]2018 - World Models]
- [2023 - From Word Models to World Models]
- [2025 - A Survey - Learning Embodied Intelligence from Physical Simulators and World Models]
  "World models are generative AI models that understand the dynamics of the real world, including physics and spatial properties! --- NVIDIA's world foundation models"
  - World Models as Neural Simulator!
  - World Models as Dynamic Models!
  - World Models as Reward Models!
** CTM(Continuous Thought Machine)和HRM(Hiearchi Reasoning Model)揭示了什么?
*** ARC-AGI,Sudoku,Maze等Benchmarks,引发了一系列的运动!
*** CTM(Continuous Thought Machine)
*** HRM(Hiearchi Reasoning Model)
- Medium上有篇博客! [[https://medium.com/intuitionmachine/the-hierarchical-reasoning-model-through-the-lens-of-quaternion-process-theory-thinking-fast-and-1fc948dad97f][The Hierarchical Reasoning Model Through the Lens of Quaternion Process Theory:Thinking Fast and Slow, Empathetically and Fluently]]
  从'Quaternion Process Theory of Cognition'角度来讲HRM作的贡献!
- 论文受人脑中分层和多时间尺度处理等的启发,提出了分层推理模型(HRM)作为一种新颖的递归架构,其能够在保持训练稳定性和效率的同时实现显著的计算深度.
- Latent Space Reasoning,即隐推理范式(即模型在内部隐藏状态空间进行推理,而不是依赖显示的语言步骤)可能是实现复杂任务推理的关键.
  - 稍早点,和Chain Of Continuous Thought相关的工作.如(CoCoNut,CoCoMix)
    ?语言与推理之间有着什么样内涵上的联系与本质上的差别?
  - 
*** 附带地,来看看Albert Gu指导的论文(H-Net),或许应当另开话题!?
* ※※※※※※※※
[[https://mlml.kaist.ac.kr/awesome#681e48cc-6b0c-4b1a-b602-32bc029572d6][Awesome X (Prof. Ahn’s Reading List)]]
** ※AI Mathematician / AI Scientist / Autoformalization※
General
2411 - Large language models surpass human experts in predicting neuroscience results
2411 - Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics
2410 - Herald - A Natural Language Annotated Lean 4 Dataset
2407 - LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover
2306 - From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought
2406 - AI-Assisted Generation of Difficult Math Questions
2405 - Metacognitive Capabilities of LLMs- An Exploration in Mathematical Problem Solving
2405 - DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data
2404 - A Survey on Deep Learning for Theorem Proving
2403 - Don’t Trust: Verify – Grounding LLM Quantitative Reasoning with Autoformalization
2403 - Machine Learning and Information Theory Concepts Towards an AI Mathematician [Y. Bengio]
2402 - REFACTOR: Learning to Extract Theorems from Proofs
2312 - NeurIPS Tutorial on Machine Learning for Theorem Proving [Video]
2310 - A New Approach Towards Autoformalization
2212 - Solving Quantitative Reasoning Problems with Language Models
2302 - Peano - Learning Formal Mathematical Reasoning
2301 - Towards Autoformalization of Mathematics and Code Correctness: Experiments with Elementary Proofs
2210 - Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs
2205 - Autoformalization with Large Language Models
20xx - A Promising Path Towards Autoformalization and General Artificial Intelligence
2009 - Generative Language Modeling for Automated Theorem
2006 - Mathematical Reasoning via Self-supervised Skip-tree Training
** ※Architectures (Backbone)※
General
2501 - What’s Next for Mamba? Towards More Expressive Recurrent Update Rules
2410 - FACTS- A Factored State-Space Framework For World Modelling
2406 - Vision-LSTM: xLSTM as Generic Vision Backbone
2405 - Aaren - Attention as an RNN
2310 - Sparse Universal Transformer
2212 - DiT - Scalable Diffusion Models with Transformers (Backbone Architecture of Sora)
2207 - Neural Networks And The Chomsky Hierarchy
2202 - GroupViT: Semantic Segmentation Emerges from Text Supervision
2202 - MaskGIT: Masked Generative Image Transformer
20xx - Theoretical Limitations of Self-Attention in Neural Sequence Models
** ※Artificial Hippocampus & Spatial Intelligence※
General
2501 - Computational Models of Hippocampal Cognitive Function - Chapter16 in [Oxford - The Hippocampus Book - 2nd - 2024]
2501 - Key-Value Memory in the Brain
2412 - Learning Hierarchical Abstractions of Complex Dynamics using Active Predictive Coding Spatial
2412 - Models of Human Hippocampal Specialization- a Look at the Electrophysiological Evidence
2411 - A Tale of Two Algorithms - Structured Slots Explain Prefrontal Sequence Memory and Are Uniﬁed with Hippocampal Cognitive Maps
2408 - Space as A Scaffold for Rotational Generalisation of Abstract Concepts
2408 - Human hippocampal and entorhinal neurons encode the temporal structure of experience
2408 - How the Human Brain Creates Cognitive Maps of Related Concepts
2408 - Why Concepts Are (probably) Vectors
2408 - Cognitive maps from predictive vision
2408 - Abstract representations emerge in human hippocampal neurons during inference
2407 - Space is a latent sequence: A theory of the hippocampus
2407 - Automated construction of cognitive maps with visual predictive coding
2407 - The Computational Foundations of Dynamic Coding in Working Memory
2406 - A recurrent network model of planning explains hippocampal replay and human behavior
2405 - Remapping revisited: how the hippocampus represents different spaces
2401 - Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments
2311 - The generative grammar of the brain: a critique of internally generated representations
2308 - Cognitive graphs: Representational substrates for planning
2302 - Replay and Compositional Computation
2209 - How to build a cognitive map
2112 - Relating Transformers to Models and Neural Representations of The Hippocampal Formation
2106 - Geometry of abstract learned knowledge in the hippocampus
2009 - Emergence of abstract rules in the primate brain
1805 - Vector-Based Navigation Using Grid-Like Representations in Artificial Agents
Spatial AI
2412 - link iconThinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces
* 工部
** FCN 中的FC是什么含义？
Fully Convolution意味着什么？
FCN 一开始就和语义分割扯上关系，怎么回事？
** Siamese and Triplet Net 在 Image Ebedding 中的作用
https://github.com/adambielski/siamese-triplet

** FPN 的目的是什么？
源自文章 "Feature pyramid networks for object detection"
各种尺度的Feature Map的来源,及相互关系,利用方式! 那么下面的术语:
- Featurized Image Pyramid: Image Pyramid 产生 Feature Map堆栈
- Single/Top Feature map: 只利用DNN的顶层Fetaure Map
- Pyramidical Feature Hierarchy: 独立利用DNN的各层Feature Map
- Feature Pyramid Networks: 是种DNN范式。按我的理解,具有"生成"的味道?
** Convolutional Activition Map 是个什么概念？
CAM: Learning Deep Features for Discriminative Localization
Grad-cam: Visual explanations from deep networks via gradient-based localization
各自说了什么事？

** Wieland Brendel and Matthias Bethge 想说什么?
*** 文章ImageNet-Trained CNNs are biased towards texture; Increasing shape bias improves accuracy and robustness.
用标题将他的内容全部说出来了.
*** 文章BagNet
进一步将上面的事说明白了!
但我将他们的代码从头实现一遍,总觉得:造做！
** Fine Grained Image Analysis
*** 纲要
**** Fine-grained image recognition
***** Fine-grained recognition by localization-classification subnetworks
****** Employing detection or segmentation techniques
****** Utilizing deep filters / activations
****** Leveraging attention mechanisms
****** Other methods
***** Fine-grained recognition by end-to-end feature encoding
****** High-order feature interactions
****** Specific loss functions
****** Other methods
***** Fine-grained recognition with external information
****** Fine-grained recognition with web data
****** Fine-grained recognition with multi-modality data
****** Fine-grained recognition with humans in the loop
**** Fine-grained image retrieval
***** Content-based fine-grained image retrieval
***** Sketch-based fine-grained image retrieval
**** Fine-grained image generation
***** Generating from fine-grained image distributions
***** Generating from text descriptions
**** Future directions of FGIA
***** Fine-grained few shot learning
***** Fine-grained hashing
***** Fine-grained domain adaptation
***** FGIA within more realistic settings
*** 论文阅读一：视觉对象的图表示及可解释性
2020 - Interpretable and Accurate Fine-grained Recognition via Region Grouping
2018 - Beyond Grids: Learning Graph Representations for Visual Recognition
2018年的文章应是这条路线的基础。基于Perceptual Grouping的观点，运用Graph Representation方法，去研究视觉对象的认知问题。
** Anchor Free Object Detection
这中间的理论问题是什么？最终追溯到“对象的表示”问题。
边界框是目前最通用的，但它提供的信息实在是太粗旷了。
什么叫合理或合适的对象表示？
*** Paper List
- 2020
  - Any More?
  - AutoAssign: Differentiable Label Assignment for Dense Object Detection.(Rejected?)
  - RepPoints V2: Verification Meets Regression for Object Detection.(*Pay Attention!*)
  - Corner Proposal Network for Anchor-free, Two-stage Object Detection.
  - HoughNet: Integrating near and long-range evidence for bottom-up object detection.
  - Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection.
  - Soft Anchor-Point Object Detection.
  - CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection.
  - SaccadeNet: A Fast and Accurate Object Detector.
  - Localization Uncertainty Estimation for Anchor-Free Object Detection.
  - Dense RepPoints: Representing Visual Objects with Dense Point Sets.(*Pay Attention!*)
  - BorderDet: Border Feature for Dense Object Detection.
  - Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.
- 2019
  - RepPoints: Point Set Representation for Object Detection.(*Pay Attention*)
  - Segmentation is All You Need.
  - FCOS: Fully Convolutional One-Stage Object Detection. (*Pay Attention*)
    [[file:./images/fcos_Architecture.png]]
  - CenterNet: Detecting Objects as Paired Points
  - CenterNet: Keypoint Triplets for Object Detection.
  - CenterNet: Objects as Points. (*Pay Attention*)
  - FoveaBox: Beyond Anchor-based Object Detector.
  - Feature Selective Anchor-Free Module for Single-Shot Object Detection.
  - ExtremeNet: Bottom-up Object Detection by Grouping Extreme and Center Points.
- 2018
  - CornerNet: Detecting Objects as Paired Keypoints.
  - An Anchor-Free Region Proposal Network for Faster R-CNN based Text Detection Approaches.
- 2016
  - You Only Look Once: Unified, Real-Time Object Detection.
  - UnitBox: An Advanced Object Detection Network.
- 2015
  - DenseBox: Unifying Landmark Localization with End to End Object Detection.
*** Arguments

** CAM和Grad-CAM的涵义:
** Graph Neural Network:
*** The Concept of GNN:
*** Graph Embeding
*** Graph Convolution:
  - Spatial Convolution
  - Spectral Convolution

** Neural Geometry and Group-CNN
Neural Geometry源自对 Visual Cortex Columns结构作几何解释。
Neural Geometry的主题在于Connection.
但[Bekkers E.J.]2020 - B-Spline CNNs On Lie Group中的工作,
将Neural Geometry与Geometric Deep Learning建立了联系.
*** [Sarti A.;Citti G.]2019 - NeuroGeometry of Perception - isotropic and anisotropic aspects
这算是对Neural Geometry一个阶段的总结.
*** G-CNN
Group convolutional neural networks (G-CNNs) are a class of neural networks that are equipped with the geometry of groups.

** Deep Implicit Layers
隐含层的概念， 有点类似隐函数的概念。
*** Neural ODEs
*** Deep Equilibrium Models
*** Differentiable optimization
** Graph Neural Diffusion(Neural PDEs)
Beltrami Flow, Ricci Flow and So On!
** Geometric deep learning
Michael M. Bronstein这个人,学习Klein Erlangen Programme的精神,搞了个纲领,叫做“Geometric Deep Learning”.
参考[Bronstein M.M.]2021 - Geometric Deep Learning - Grids, Groups, Graphs, Geodesics, and Gauges


*** Geometric Priors
- Symmetry
- Scale Separation
*** Geometric Domains
*** Geometric Deep Learning Models
** FEP, VAE and EBM
我想还是以几篇Review来展开吧!
*** [Buckley C.L.]2017 - The free energy principle for action and perception: A mathematical review
*** [Clark A.]2013 - Whatever next? Predictive brains, situated agents, and the future of cognitive science
*** [Now]2019 - An Introduction to Variational Autoencoders
** Modern Hopfield Network这条线路又意味者什么?
Energy-Based-Model(EBM)的要点:
 - "Capture Dependencies through an energy function"
 - "but, the energy is used for inference, not for learning"
 - "Probabilistic models are a special case of EBM.","but, EBM is more"
   - "EBM gives more flexibility in the choice of the scoring function."
   - "More flexibility in the choice of objective function for learning"
   - "From energy to probability: Gibbs-Boltzmann distribution"
Hopfield网络是联想记忆,记忆Patterns就定义了能量地势,地势的梯度下降给出「回忆」的结果.
*** Modern Hopfield Network 开始于Binary States的Dense Associative Memory(DAM:[Krotov D.]2016 - Dense Associative Memory for Pattern Recognition.)
*** 继之以Continuous States的进展!([Demircigil, M;...]2017 -  On a model of associative memory with a huge storage capacity)
*** 发现Modern Hopfield Network和Transformers的关系!
Attention is all you need! && Hopfield is all you need!
*** Krotov和Hopfield出来澄清一些事!!!([Krotov D.]2020 - Large Associative Memory Problem in Neurobiology and Machine Learning)
[Demircigil, M.;]定义的能量函数,不符合生理学或生物学道理!从微观生物学的角度,也可以给出新的能量函数,等等!
*** [Krotov D.]在IBM在搞名堂! Hiearchical Associative Memory && Energy Transformers
**** [Krotov D.]2021 - Hierarchical Associative Memory
**** [Krotov D.]2022 - Energy Transformers
*** 能量函数,是否在Associative Memory和Probabilistic Modeling之间建立了桥梁？（2024 - Bridging Associative Memory and Probabilistic Modeling）
人工智能的两个基础领域,是否有融通的可能？
** E-Patient/Digital-Patient & E-Medicine/Digital-Medicine ~~> Computational Anatomy(计算解剖学) & Computational Physiology(计算生理学)
*** Computational Anatomy
The objective of Computational Anatomy (CA) is the modeling and analysis of biological variability of the human anatomy.
Typical applications cover
- the simulation of average anatomies and normal variations,
- the discovery of structural differences between healthy and diseased populations,
- and the detection and classification of pathologies from structural anomalies.

Studying the variability of biological shapes is an old problem (cf. the remarkable book “On Shape and Growth” by D’Arcy Thompson).
Significant efforts have been made since that time to develop a theory for statistical shape analysis(统计形状分析).
Despite all these efforts, there is a number of challenging mathematical issues which remain largely unsolved in general.
A particular issue is the computation of statistics on manifolds which can be of infinite dimension (e.g the group of diffeomorphisms).

There is a classical stratification(分层) of the problems into the following 3 levels:
- construction from medical images of anatomical manifolds of points, curves, surfaces and volumes;
- assignment of a point to point correspondence between these manifolds using a specified class of transformations (e.g. rigid, affine, diffeomorphism);
- generation of probability laws of anatomical variation from these correspondences.
*** Computational Physiology
The objective of Computational Physiology (CP) is to provide models of the major functions of the human body and numerical methods to simulate them.
The main applications are in medicine and biology,
where CP can be used,for instance,
- to better understand the basic processes leading to the apparition of a pathology,
- to model its probable evolution
- and to plan, simulate, and monitor its therapy.


There is a hierarchy of modeling levels for CP models of the human body:
- the ﬁrst level is mainly geometrical, and addresses the construction of a digital description of the anatomy, essentially acquired from medical imagery;
- the second level is physical, involving mainly the biomechanical modeling of various tissues, organs, vessels, muscles or bone structures;
- the third level is physiological, involving a modeling of the functions of the major biological systems
  (e.g. cardiovascular, respiratory, digestive, central or peripheral nervous, muscular, reproductive, hormonal, etc.)
  or some pathological metabolism (e.g. evolution of cancerous or inﬂammatory lesions, formation of vessel stenoses, etc.);
- a fourth level would be cognitive, modeling the higher functions of the human brain.

这个分层,是从Visible Human 到Physioloical Human的分层!!
*** 从现在情况来看,深度学习神经网络在这些方面,有很多应用!
** DETR 给Object Detection带来的新范式
Detection Transformer(DeTr): End-To-End by Transformers.
Anchor Based --> Anchor Free --> End-to-End!!
*** How does DeTr do it?
**** Transformer
**** Learnable Object Queries
**** Set-Prediction

*** Some Rethinking
**** Effective Transformer.
Deformable-DeTr(重点看看,打开DeTr的正确方式?),UP-DeTr等等
***** What makes for an effective attention mechanisms?
***** How to aggregate multi-scale features into attention mechanisms?
***** How to train the transformer effectively?
这方面的研究,和在ViT上的很多研究是一致的.
**** Deformable Convolution NetWork 和 Transformer之间的深沉关系.(?都在解决"long-range dependencies and adaptive spatial aggregation"问题?)
InternImage, 按照ViTs的架构,来设计基于DCN的架构, 居然也取得了SOTA.
**** What makes for a sparse object detection methods.
Sparse R-CNN等
**** New label-assignment methods.
OTA等,
***** Considering label assignment from a global view.
**** What makes for a end-to-end object detection?
OneNet等
主要还是和label-assignment有关.具体的说,还要考虑什么因素?
**** How to extends to segmentation task?
***** SOLQ(Segmenting Object As Queries)等,还有对Box的考虑
***** MaX-DeepLab则开启了基于Mask Transformer的端到端(全景)分割框架.(MaX指Mask XFormers).
似乎有这样一条线索: *Box Based + Mask Classification --> *Box Free + *Mask Classification*
MaX是mask xformer的缩写! 可以写成一个公式!  SoftMax(Features x ObjectQueries.T) [维度为 (HW, D) x (D, N) --> (HW, N)].想想!
- MaX-DeepLab --> kMaX-DeepLab
  MaX-DeepLab的主要贡献是, 在做Mask Classification时,去掉了对Box(如Mask R-CNN)的依赖.
  这样作的好处,我猜,在于Segmentation 不再是 Detection的后继步骤.如同Faster R-CNN 与 Mask R-CNN的关系!
  但其Loss函数还是依赖很多.
  kMaX-DeepLab在MaX-DeepLab的基础上,讨论了cross-attention和cluster之间的关系.
- MaskFormer(For Semantic segmentation) --> Mask2Former (For panoptic segmentation) --> OneFormer (Universe image segmentation) --> OMG-Seg()
  + MaX(Mask XFormer)
    MaX垫定了理论形式.
    MaskFormer应当是后继工作!
  + Mask Classification 理论上,可以将各种分割任务进行统一.
  + Mask3D 是Mask Classification上的拓展.其他领域的扩展,可能要关注下.
    [2023 - Mask3D for 3D Semantic Instance Segmentation]
- SegFormer (For semantic segmentation)--> Panoptic SegFormer (For panoptic segmentation)
- DiNo (For object detetction) --> Mask DiNo (Can object detection and segmentation be unified)
  + 这是在找话题吗?还是真见地?最终,DiNo家族在往Mask Classification靠拢!

**** How to apply for transfer learning or domain adaption task?
*** DINO家族(此DINO非彼DiNo,不道德的取名方式)
DINO means "Detr with Improved deNoising anchOr boxes"
DAB-DeTr --> DN-DeTr --> DiNo --> (Mask DiNo, MP-Former) --> OpenSeeD

** 关于Set-Prediction Problem 还可看看下述文章
Deep Sets
Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks
Generative Adversarial Set Transformers
Conditional Set Generation with Transformers
** DiNo 给ViT的带来的新范式
看来FaceBook或MetaAI在这个方向,有不错的努力.前面还有项工作,DeiT,也作了不少努力!
文章2021 - DINO: Emerging Properties in Self-Supervised Vision Transformers
文章2023 - DINOv2: Learning Robust Visual Features without Supervision (附加 文章2023 - Vision Transformers Need Registers)

DiNo means "self-Distillation with No labels"
Dino是第一个在ViT架构上作自监督学习的!
之前,都是在CNN架构的网络上作自监督学习的.
关键问题:学习方法对ViT架构为什么有如此大的影响?导致自监督学习在目前是个研究热点!
*** 回顾一下Self-Supervised的历史
(有必要的话,可以考虑一下KD(Knowledge Distillation), 和Self-Supervised之间的碰撞)
**** Pretext Task : "Relative Location","Colorization","Context Encoder","Rotation Prediction"等主题文章.
"Pretext Task":所谓的"前置任务".
**** Contrastive Learning :
***** "MoCo(v1,v2,v3)":MoCo 这系列论文，将之前的对比学习,总结成字典查找的框架,再基于此提出 MoCo(Moment Contrast).
***** "SimCLR":
***** "BYOL(Boot your own latent)":
***** "SwAV(Swapping Assignments between multiple Views )":
***** "SimSiam": SimSiam 这篇论文则是对上述多篇论文进行了总结,并且化繁为简.
***** "BarlowTwins":
***** "DiNo": "self-supervised learning as a form of Mean Teacher self-distillation with no labels."
这句话怎么理解?

**** Masked Image(Data) Modeling:
ViT, 为NLP中的掩码学习机制，应用到视觉学习中， 打开了一扇大门！
***** BEiT(Bidirectional Encoder representation from Image Transformers):
***** MAE(Masked Autoencoders):
***** SimMIM:
***** MaskFeat:
*** Dino
**** 网络架构: Self Distillation Architecture
采用Momentum Tearcher模型.思想来自"Momentum Contrast for Unsupervised Visual Representation Learning".
据说,可有效解决Mode Collapse 问题!
**** 数据增强
DINO 中最核心的数据采样策略便是图像裁剪，这也是自监督学习领域应用非常广泛的主策略之一.
一般来说，我们可以将裁剪后的图像分为两种:
- Local views: 即局部视角，也称为 small crops，指的是抠图面积小于原始图像的 50%.
- Global views: 即全局视角，也称为 large crops，指的是抠图面积大于原始图像的 50%.
在 DINO 中，学生模型接收所有预处理过的crops图，而教师模型仅接收来自global views的crops图。
**** Loss函数
教师和学生得分的Cross Entropy!
**** Centering and Sharpening (可以研究一下,处理模式崩塌的技术)
在自监督学习中，mode collapse 是指网络的学习过程中出现了多样性减少的现象.
具体来说，当网络学习到一组特征表示时，往往会出现多个输入数据映射到相同的特征表示的情况，这就是所谓的模式崩塌.

这种现象通常是由于网络在优化过程中陷入了局部最优解.
只能考虑到一部分数据的特征表示,而忽略了其它数据样本的模式和特征,从而导致了多样性缺失的现象,因此会对模型的鲁棒性产生很大的负面影响.
*** DinoV2(偏现有技术的灵活综合运用!)
改善内容:
**** 数据集处理:
去燥(不合格的数据),去重(参考:A self-supervised descriptor for image copy detection) 得到数据集LVD-142M.
**** 方法
***** Discriminative Self-Supervised Method: 可认为是下述(DINO, iBOT, SwAV)三种方法的综合/组合?
- DINO: <==> Image Level Objective
- iBOT: <==> Patch Level Objective
- Sinkhorn-Knopp centering(SwAV):这个方法的核心思想是通过正则化来使学生和教师网络在特征表示上更加接近.
  (出自Dino作者早期的一些工作(其实,我发现DiNoS的工作都和他在Contrastive Learning,Deep Clustering方面的工作有关):
   "2020 - Unsupervised learning of visual features by contrasting cluster assignments",
   "2018 - Deep Clustering for Unsupervised Learning of Visual Features"
  )
- Untying head weights between both image and patch objectives. DiNo易过拟合,iBot易欠拟合.
- KoLeo regularizer:
- Adapting the resolution:
***** Efficient Implementation
这得学习源代码了!

** CSSP: Column Subset Selection Problem
文章Select to Better Learn: Fast and Accurate Deep Learning using Data Selection from Nonlinear Manifolds
** Variants of Transformer
我认为这可能是个重要的话题.Transformer本身,毕竟不是高效的.
[[file:images/VariantsOfTransformer.jpg][VariantsOfTransformer]]
Sparse transformer, LongFormer, Switch Transformers :
- *Explicit Sparse Transformer: : Concentrated Attention Through Explicit Selection*
- *Longformer: The Long-Document Transformer*
- *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*
Transformer-XL, Star-Transformer:
- *Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context*
Routing Transformer, Linformer, Big Bird:(其实还是稀疏路线)
- *Efficient Content-Based Sparse Attention with RoutingTransformers*
- *Linformer: Self-Attention with Linear Complexity*
- *Big Bird: Transformers for Longer Sequences*

** 理解大语言模型的一些文章
*** Sebastian Ruder这几年老作评论:
**** 2018年, Sebastian Ruder做了一个回顾,比较权威.
- 2001 - 神经语言模型
- 2008 - 多任务学习
- 2013 - 词嵌入
- 2013 - NLP 神经网络
- 2014 - sequence-to-sequence 模型
- 2015 - 注意力机制
- 2015 - 基于记忆的网络
- 2018 - 预训练语言模型
**** 2021年, Sebastian Ruder提到的NLP热点
- Universal Models
- Massive Multi-task Learning
- Beyond the Transformer: (Perceiver..)
- Prompting
- Efficient Methods
- Benchmarking
- Conditional Image Generation
- ML for Science
- Program Synthesis
- Bias
- Retrieval Augmentation
- Token-free Models
- Temporal Adaptation
- The Importance of Data
- Meta-learning
*** NLP的10片主干文章
**** 理解大语言模型的结构和任务
- Neural Machine Translation by Jointly Learning to Align and Translate (2014)
  此文,引入了Attention Mechanisms,成为引入Transformer的动机!
- Attention Is All You Need (2017)
  See [[./images/NLP/Attention.jpg]]
  在原始的 Transformer 模型之后，大语言模型研究开始向两个方向分化:
  - 基于编码器结构的Transformer模型用于预测建模任务,例如文本分类;
  - 而基于解码器结构的Transformer模型用于生成建模任务,例如翻译、摘要和其他形式的文本内容生成。
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
  BERT是基于解码器的模型结构!
- Improving Language Understanding by Generative Pre-Training (2018) -- GPT
  GPT是基于解码器的模型结构!
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)
**** 规模法则和模型效率提升
- Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)
- Cramming: Training a Language Model on a Single GPU in One Day (2022)
- Training Compute-Optimal Large Language Models (2022)
**** 对齐——引导大语言模型完成训练目标
- Training Language Models to Follow Instructions with Human Feedback (2022) -- InstructGPT!
- Constitutional AI: Harmlessness from AI Feedback (2022)
**** 关于人类反馈的强化学习(RLHF)
- Asynchronous Methods for Deep Reinforcement Learning (2016)
- Proximal Policy Optimization Algorithms (2017)
- Learning to Summarize from Human Feedback (2022)
*** NLP中在PTMs思想下的一般性的神经网络架构
Pre-trained Models for Natural Language Processing: A Survey(2020)
See [[./images/NLP/Embedding.jpg]]
应当说,这篇文章把PTM总结得比较好!
*** Prompting的花样年华
Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(2021)
四个NLP的范式,See: [[./images/NLP/prompting.jpeg]]
这篇综述,让Prompting红得发紫.其实这篇文章,应当是个典范.一片综述,让一个模糊概念有了清楚的正式定义,并指明了这个领域的研究范畴.
形式化地讲,在Prompt范式下,需要通过以下三个步骤建立从输入到输出的PipeLine.
- Prompt addition:
- Answer search:
- Answer mapping:
由是而有下面五方面的工作.
- Pre-trained Model Choice
- Prompt Engineering
- Answer Engineering
- Expanding the Paradigm
- Prompt-Based Training Strategies
*** Context Engineering的兴起!
!!!怎么说呢?

** Deformable Convolution 和 Transformer的一个局部较量!
商汤的2023 - InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions.
看来这篇文章将DCN推到一个可理论探索的地步!
** Mamba代表State Space Model(SSM)走入和Transformer竞争的行列!这里面可能真有戏!
*** Mamab的道路:
**** Mamba
但前景如何? 事实上,"An Attention Free Transformer"一文,开启一些新方向!(RWKV,GateLoop,HGRN等等)
SSM -> Hippo -> LSSL -> S4 -> Mamba
这里充满者大量的技术细节调整!有理论性的,首当HiPPO和Mamba, LSSL次之! S4,S4D,S5再次之!!
[[file:images/ssm-hippo-s4-mamba.jpg][SSM-To-Mamba]]
 - [Gu A.;...]2020 - HiPPO - Recurrent Memory with Optimal Polynomial Projections
   这篇文章,处理了长期依赖的问题( Addressing Long-Range Dependencies)!应当说,还包括在线记忆(Online Memory)问题.
   HiPPO是个关键,有一定的理论意义!!!
   这篇文章的表达,实在不敢恭维!
   ~说句老实话,没看懂后期的工作,和他有什么真实的关系,而更多的是启发性的关系!即什么样的矩阵具有记忆能力!~
 - [Gu A.;...]2021 - LSSL - Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers
   之前: "Continuous State Space Model" -> "Discrete-time SSMs(Inference SSMs): The recurrent representation"
   而这个LSSL,发现: => "Training SSMs: The Convolutional Representation"
   _这是一篇过渡文章,形成了SSM的双视角!("the dual recurrent-convolutional forms")._
   "SSM convolution kernel - SSK" or "Kyrov Function", 可以探索一下!
 - [Gu A.;...]2022 - S4 - Efficiently Modeling Long Sequences with Structured State Spaces
   这篇文章的贡献,主要集中在计算方法的优化!!!
   S4,所谓的结构化方法
   - the continuous representation: Hippo Matrix的选择.
   - the recurrent representation: 离散方法的选择,(选择跟随Bilinear Method).
   - the convolutional representation: 可高效计算Kyrov Function(即那个SSK函数)的参数矩阵是对角化矩阵.
     矩阵的共轭等价变化,可得到对角矩阵.
     问题是,只有正规矩阵(Normal Matrix)可以对角化.而Hippo Matrix不满足条件!是否可将问题弱化呢?
     S4的贡献在于,将Diagonal Matrix 放宽到 DPLR Matrix(Diagonal Plus Low Rank Matrix).
     并且,发现HIPPO Matrix中的HiPPO_LegS,HiPPO_LegT,HiPPO_LagT等等的矩阵,满足NPLR(Normal Plus Low Rank)条件.
     NPLR和DPLR是共轭的!!!
     看来,如果要深入,需要一些矩阵计算方法的背景了.
     看了一堆相关文章DSS,S4D等等,发现有点技术灌水的味道!!!(它们在互相引用!!!)
 - [CMU]2023 - Mamba - Linear-Time Sequence Modeling with Selective State Spaces
   - "We argue that a fundamental problem of sequence modeling is compressing context into a smaller state!!"
     这是一个什么观点? "状态应当将上下文压缩进来!".
     - 纯粹的LTI models,天生有什么短板?
       "The Selective Copying task", "The Induction Heads task"等任务揭示了什么? LTI模型的状态,缺少"context"在其中!!!
       In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state:
       - efficient models must have a small state,
       - while effective models must have a state that contains all necessary information from the context.
     - 解决问题的方法之一,就是"Selection". 但,什么是"Selection"?
       - 但Selection,具体地说, selective scan,又破坏了S4中的卷积递归双面性!!!
         This “Selective Scan” operation now, does not hold the dual property of convolution and recurrence.
         Hence, Mamba relies on recurrence only.
         不过此时,有并行算法存在!
**** Gu A.在他的博士论文里,对基本概念讲得更清楚!
Seq-To-Seq是很重要的一个抽象理论模型,用SSM来解决Seq-To-Seq问题,这是开窍点吗?
*** Mamba的朋友们!或Transformer的竞争者门!
**** RNN,LSTM,GRU还是要比较认识一下!
[[file:images/Comparison-of-RNN-LSTM-and-GRU-neural-network-structures.png][图: RNN vs LSTM vs GRU]]
[[file:images/Structure of the LSTM cell and equations.png][图: LSTM Cell]]
[[file:images/Structure of the GRU cell and equations.png][图: GRU Cell]]
**** Mamba的朋友们!(也就是所谓的Transformers的后继们)!Titans带来的进步和总结:
- Linear RNN ~~> Linear Attention 等等,这里可能有些简单的分类问题,可以不必纠结?
  ...,Linear Transformer,RetNet,RWKV,DeltaNet,...,可以去看看,科学空间的几篇文章,
  - [线性注意力简史：从模仿、创新到反哺] -- 2025
  - [Google新作试图“复活”RNN：RNN能否再次辉煌？] -- 2023
- [[ΩΔΔ;MetaAI]2024 - TTT - Learning to (Learn at Test Time) - RNNs with Expressive Hidden States]
  - 线性Attention从最初的简单模仿Softmax Attention，到引入静态衰减因子乃至“data-dependent decay”，已经形成了自身的特色并在不少任务上发挥价值。
    然而，这些进展多数是靠人工凭经验设计出来的,
    ~我们不禁要问:有没有更上层的原则来指导线性Attention甚至是一般的序列模型(Token-Mixer)的设计?~
    ?用Linear Attention来构建RNN?
  - 什么是隐涵状态?隐涵状态是可表达的!隐涵状态时可更新的!
- [[ΔΔ]2024 - DeltaNet - Parallelizing Linear Transformers with the Delta Rule over Sequence Length]
- [[ΣΔΔ;Google]2024 - Titans - Learning to Memorize at Test Time]
  - ~在Titan的研究中,提到FWP(Fast Weight Controller/Programmer).忽然感受到古老思想背后的直觉,还是有深刻的洞察力的!~
      FWP应当就是那位Transformer的失落者(Jürgen Schmidhuber)开辟的路线?!近期,他们的成果确实比较多(比如,World Models!)!
      [Fast weight programming and linear transformers: from machine learning to neurobiology]
      FWP & DeltaRule & Linear Transformer(Linear Attention) ...
  - 在Titans这篇文章里,Attention,Linear Attention,Association Memory,Memory等等,进行了集中反思!
  - ~同时Titans的这份研究~,似乎落入某个范式的转变:(Latent Reasonging???)
    从"无状态的计算工具"向"有记忆的智能体"的范式转变!
    绕开微调,在模型外围找解决方案,探索如何让AI系统真正具备持续学习和自我改进的能力!
    Titans可能是名单的开头!
    Training-Free GRPO
    ReasoningBank
    Agentic Context Engineering
    ... Anything else?
**** 在本身并不是一维的数据领域,运用Mamba之类的技术,需要将数据序列化!
~高维数据,如何有一个好的序列化?~
***** Space Filling Curves
***** Tree Topology
- "古老的世界里,有这么一个传奇!"
  Tree-Filtering [2013 - Tree Filtering: Efficient Structure-Preserving Smoothing With a Minimum Spanning Tree]
- Tree-Topology [2025 - GrootVL: Tree Topology is All You Need in State Space Model]
  思路还是挺好的!

** H-Net,Albert Gu对Hiearchie的思考！
*** [2025 - Dynamic Chunking for End-to-End Hierarchical Sequence Modeling - v2]
Chunking Theory的涵义？
*** Albert Gu的Blog [https://goombalab.github.io/blog/2025/hnet-past/]详细谈了这个过程,及背后的动机!
什么是"Primtives"! "Hierarchy"如何可作为"Differentiable Primitives"?
~事实上,层次这个概念,至少在深度学习这个领域,被CNN的Pyramid Hierachie结构观,给带偏了.层次间的"互动",丢失了!~
** 有必要从一个理论的高度来看待近期的一些动向!
[2025 - A Neuro-Inspired Computational Framework for AGI - Predictive Coding Active Inference and Free Energy Minimisation]
这是一片好的理论建构文章.思想、理论和启发!
- 很多话题,在下面展开!TiTans,HRM,CTM,H-Net等等文章的思想支撑,都应当和这篇文章代表的路线,进行比较研究!
- 还有以前没搞懂的,如GFlowNet,BriLLM...
** 从Perceptual Grouping角度看基础模型的其他路线
当将Transformer运用到2D,3D的数据上时,就会发现Space is sequences!
这就面临一个问题,How to sequence the space!But, sequencing is triggered by grouping or selection with one by one!
Set => Subsampled Or hiearchied Set(Clustering and Feature_Aggregating) => Grouping / Sequencing
*** "GroupViT: Semantic Segmentation Emerges from Text Supervision"
有点学CLIP,来grouping!(可以了解一下)
*"DVT:Not All Images are Worth 16x16 Words - Dynamic transformers for Efficient Image Recognition"*
这常被提及,但和Grouping的路线,还是不靠边!
*** "Image as set of points"的工作:卷积,注意力之外的另一条视觉表示路线!
(PointMLP和这篇文章是同一作者)
We introduce ~Context Cluster~, a novel feature extraction paradigm for visual representation.
We hope our ~Context Cluster~ can be considered as a novel visual representation method ~in addition to~ convolution and attention.
这篇论文的意义,在于她的宣言!
- "Image as set of points"说她受到PointMLP的影响,那么PointMLP代表什么路线呢?
  "PointNet -> PointNet++ -> PointMLP: 局部几何特征 + ~Set~. 估计可以这样说!!!"
- "Image as set of points" 真正的贡献,不在于聚类,而在于聚类后用"Aggregating -> Dispatching"操作,让类中的点交互!!
  ~"Context Cluster -> Feature Aggregating -> Feature Dispatching" 这样一个聚散过程!!!~
  "聚散"是不同于卷积和注意的特征提取操作!这才是她说的贡献.既一种新的特征表示的提取方法!
*** Self-Attention in transformers is not Attention!
**** [John K. Tsotsos]2023 - Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention
没怎么读这篇文章.但Tsotsos出来发话,可能还是有不少意图的.

*** "Perceptual Group Token"一文是否该值的重视!
**** 为什么作者认为"Image as set of points"里的工作,和自己的工作最相关!
**** 一系列的Mamba/S4的灌水文章,似乎体现的都是这篇文章的思想!!!
** Neural ODE这篇文章到底有什么可以高看的地方？
为什么“Mit的评审意见认为这是一个可以和Ian Goodfellow的GAN相媲美的‘a radical new design’”
另外,我看到一个我未深入思考的领域,神经网络在经典理论科学的应用!
** VitPose 和 TokenPose
TokenPose是VitPose不出来之前的东西!估计如此!
** Perceiver:为何称之为General Perception?
启发Perceiver的,有一项工作,Set Transformer[Set Transformer - A Framework for Attention-based Permutation-Invariant Neural Networks].
[Perceiver: General Perception with Iterative Attention]
[Perceiver IO: A General Architecture for Structured Inputs & Outputs]
[Perceiver AR: General-purpose, long-context autoregressive modeling with Perceiver AR]
实质上,真还是忽略了他的意义!扯远点,Mask2former里,好像也看到某种类似的痕迹!可以看看"Dyn-Perceiver"中的内容!
Cross-Attention其实还是有深意的!谁Q,谁K,V,应当有思想!
Perceiver应当是Latent-Space的构造器!
** SAM Oneformer SegGPT SEEM这些并行工作意味着什么?
综述:[2023 - A Comprehensive Survey on Segment Anything Model for Vision and Beyond - v2]
